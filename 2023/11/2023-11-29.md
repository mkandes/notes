# Gaudi2 Benchmarks on Intel Devloper Cloud

We've completed an extensive set of benchamrk tests running the MLPerf Image Classification Training Benchmark 
on Gaudi2 using TensorFlow to train ResNet-50 on the ILSVRC2012 dataset. This replicated a subset of the 
acceptance tests previously run for Voyager. The notes shown below are my first steps in setting up the PyTorch 
version of the benchamrk on the same Gaudi2 node following the Habana Model Reference directions here.

https://github.com/HabanaAI/Model-References/tree/master/PyTorch/computer_vision/classification/lightning/resnet

```
root@sdsc-gaudi2:~# docker pull vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/pytorch-installer-2.0.1:latest
latest: Pulling from gaudi-docker/1.12.0/ubuntu20.04/habanalabs/pytorch-installer-2.0.1
edaedc954fb5: Already exists 
38e957dbcd7b: Already exists 
71922d9a3fc0: Already exists 
6c8ea093685e: Already exists 
1ad9dbb112cc: Already exists 
4efc0eb93b12: Already exists 
9be0346a93a1: Already exists 
940c5fa77f7a: Already exists 
35d027ae9fdd: Already exists 
ebb2500966e3: Pull complete 
9507c1a5f745: Pull complete 
5f65391158ac: Pull complete 
b97090f9c67d: Pull complete 
4f4fb700ef54: Pull complete 
Digest: sha256:0b2c79c6f01992588d39a7dcaea1497387148bb5eb5f6f6fa6ae91129fcbad3f
Status: Downloaded newer image for vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/pytorch-installer-2.0.1:latest
vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/pytorch-installer-2.0.1:latest
root@sdsc-gaudi2:~# history | grep docker | grep run
   33  docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --mount type=bind,source="${PWD}",destination="${PWD}" vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
   42  docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --mount type=bind,source="${PWD}",destination="${PWD}" vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
   48  docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --mount type=bind,source="${PWD}",destination="${PWD}" vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
   71  docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --mount type=bind,source="${PWD}",destination="${PWD}" vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
  125  docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
  216  docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
  247  docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
  282  docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
  374  history | grep "docker run"
  375  docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
  379  history | grep "docker run"
  904  docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.13.0:latest
 1012  history | grep docker | grep run
root@sdsc-gaudi2:~# docker run --name test --runtime habana --network host --mount type=bind,source="${HOME}",destination="${HOME}" --cap-add 'sys_nice' -it vault.habana.ai/gaudi-docker/1.12.0/ubuntu20.04/habanalabs/pytorch-installer-2.0.1:latest
root@sdsc-gaudi2:/# cd ~/
root@sdsc-gaudi2:~# ls
2114  list_affinity_topology_bare_metal.sh  snap  software
root@sdsc-gaudi2:~# cd software/
habana/     pytorch/    tensorflow/ 
root@sdsc-gaudi2:~# cd software/
habana/     pytorch/    tensorflow/ 
root@sdsc-gaudi2:~# cd software/
habana/     pytorch/    tensorflow/ 
root@sdsc-gaudi2:~# cd software/habana/Model-References/
CONTRIBUTING.md  .gitignore       PyTorch/         
.git/            MLPERF3.0/       README.md        
.github/         MLPERF3.1/       TensorFlow/      
root@sdsc-gaudi2:~# cd software/habana/Model-References/
CONTRIBUTING.md  .gitignore       PyTorch/         
.git/            MLPERF3.0/       README.md        
.github/         MLPERF3.1/       TensorFlow/      
root@sdsc-gaudi2:~# cd software/habana/Model-References/PyTorch/computer_vision/classification/lightning/
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning# ls
resnet
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning# cd resnet/
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# ls
datamodules.py  README.md  requirements.txt  resnet50_PTL.py
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# pip install --user -r requirements.txt 
Requirement already satisfied: lightning==2.0.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.0.6)
Requirement already satisfied: lightning-habana==1.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.0.1)
Collecting torchmetrics==0.10.0
  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 529.2/529.2 kB 25.5 MB/s eta 0:00:00
Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (4.66.1)
Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (1.23.5)
Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (6.0)
Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2023.9.2)
Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (5.10.1)
Requirement already satisfied: backoff<4.0,>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2.2.1)
Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (13.5.3)
Requirement already satisfied: inquirer<5.0,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (3.1.3)
Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2.0.9)
Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (5.9.5)
Requirement already satisfied: fastapi<2.0,>=0.92.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.103.1)
Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2.0.5)
Requirement already satisfied: pydantic<2.1.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2.0.3)
Requirement already satisfied: requests<4.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2.31.0)
Requirement already satisfied: websockets<13.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (11.0.3)
Requirement already satisfied: packaging<25.0,>=17.1 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (23.1)
Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.9.0)
Requirement already satisfied: arrow<3.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (1.2.3)
Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (4.12.2)
Requirement already satisfied: uvicorn<2.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.23.2)
Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (6.5.0)
Requirement already satisfied: lightning-cloud>=0.5.37 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.5.38)
Requirement already satisfied: starsessions<2.0,>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (1.3.0)
Requirement already satisfied: click<10.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (8.1.7)
Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (3.1.2)
Requirement already satisfied: croniter<1.5.0,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (1.4.1)
Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (4.8.0)
Requirement already satisfied: dateutils<2.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.6.12)
Requirement already satisfied: python-multipart<2.0,>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.0.6)
Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (1.6.3)
Requirement already satisfied: starlette in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (0.27.0)
Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from lightning==2.0.6->-r requirements.txt (line 1)) (2.0.1a0+gitf520939)
Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from arrow<3.0,>=1.2.0->lightning==2.0.6->-r requirements.txt (line 1)) (2.8.2)
Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning==2.0.6->-r requirements.txt (line 1)) (2.5)
Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from dateutils<2.0->lightning==2.0.6->-r requirements.txt (line 1)) (2023.3.post1)
Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from deepdiff<8.0,>=5.7.0->lightning==2.0.6->-r requirements.txt (line 1)) (4.1.0)
Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.8/dist-packages (from fastapi<2.0,>=0.92.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.7.1)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.8.5)
Requirement already satisfied: python-editor>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.0.4)
Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.8/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.6->-r requirements.txt (line 1)) (4.0.5)
Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.20.0)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2<5.0->lightning==2.0.6->-r requirements.txt (line 1)) (2.1.3)
Requirement already satisfied: pyjwt in /usr/local/lib/python3.8/dist-packages (from lightning-cloud>=0.5.37->lightning==2.0.6->-r requirements.txt (line 1)) (2.8.0)
Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from lightning-cloud>=0.5.37->lightning==2.0.6->-r requirements.txt (line 1)) (1.16.0)
Requirement already satisfied: pydantic-core==2.3.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2.1.0,>=1.7.4->lightning==2.0.6->-r requirements.txt (line 1)) (2.3.0)
Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2.1.0,>=1.7.4->lightning==2.0.6->-r requirements.txt (line 1)) (0.5.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<4.0->lightning==2.0.6->-r requirements.txt (line 1)) (2023.7.22)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<4.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.2.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<4.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.4)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich<15.0,>=12.3.0->lightning==2.0.6->-r requirements.txt (line 1)) (2.16.1)
Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from rich<15.0,>=12.3.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.0.0)
Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from starsessions<2.0,>=1.2.1->lightning==2.0.6->-r requirements.txt (line 1)) (2.1.2)
Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.6->-r requirements.txt (line 1)) (3.12.4)
Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.12)
Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn<2.0->lightning==2.0.6->-r requirements.txt (line 1)) (0.14.0)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (4.0.3)
Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (23.1.0)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.3.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (6.0.4)
Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.9.2)
Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.8/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.1.3)
Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.3.0)
Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.8/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning==2.0.6->-r requirements.txt (line 1)) (0.2.6)
Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning==2.0.6->-r requirements.txt (line 1)) (0.1.2)
Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.8/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning==2.0.6->-r requirements.txt (line 1)) (68.2.2)
Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning==2.0.6->-r requirements.txt (line 1)) (1.3.0)
Installing collected packages: torchmetrics
Successfully installed torchmetrics-0.10.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# ls
datamodules.py  README.md  requirements.txt  resnet50_PTL.py
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# docker ps
bash: docker: command not found
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# history | grep PYTHONPATH
   36  export PYTHONPATH=$PYTHONPATH:/root/Model-References
  161  echo PYTHONPATH
  162  echo $PYTHONPATH
  229  export PYTHONPATH='/root/software/habana/Model-References:/usr/lib/habanalabs/:/root'
  263  export PYTHONPATH='/root/software/habana/Model-References:/usr/lib/habanalabs/:/root'
  891  export PYTHONPATH='/root/software/habana/Model-References:/usr/lib/habanalabs/:/root'
 1012  history | grep PYTHONPATH
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# !891
export PYTHONPATH='/root/software/habana/Model-References:/usr/lib/habanalabs/:/root'
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.1 0.01 0.001 0.0001 \
> python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.1 0.01 0.001 0.0001 --custom_lr_milestones 0 30 60 80 --hpus 1 --max_train_batches 500 --epochs 5 ^C
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_path /root/2114 --autocast --custom_lr_values 0.1 0.01 0.001 0.0001 --custom_lr_milestones 0 30 60 80 --hpus 1 --max_train_batches 500 --epochs 5
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[0, 30, 60, 80], custom_lr_values=[0.1, 0.01, 0.001, 0.0001], data_path='/root/2114', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=1, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
Missing logger folder: /root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/lightning_logs
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Traceback (most recent call last):
  File "resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 934, in _run
    call._call_setup_hook(self)  # allow user to setup lightning_module in accelerator environment
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 83, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 164, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/datamodule/datamodule.py", line 166, in setup
    self.dataset_train, self.dataset_val = load_data(
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/datamodule/datamodule.py", line 98, in load_data
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# ls -lahtr
total 60K
-rw-r--r-- 1 root root  17K Nov 21 20:15 resnet50_PTL.py
-rw-r--r-- 1 root root   62 Nov 21 20:15 requirements.txt
-rw-r--r-- 1 root root 4.3K Nov 21 20:15 README.md
-rw-r--r-- 1 root root 7.3K Nov 21 20:15 datamodules.py
drwxr-xr-x 3 root root 4.0K Nov 21 20:15 ..
drwxr-x--- 2 root root 4.0K Nov 29 18:19 .graph_dumps
drwxr-xr-x 2 root root 4.0K Nov 29 18:19 __pycache__
drwxr-xr-x 5 root root 4.0K Nov 29 18:19 .
drwxr-xr-x 3 root root 4.0K Nov 29 18:19 lightning_logs
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_paroot@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_paroot@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/clasroot@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/clasroot@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# ph_size 256 --data_path /root/2114 --autocast --cusython3 resnet50_PTL.py  --batch_size 256 --data_path /root/2114 --autocast --custom_lr_values 0.1 0.01 0.001 0.0001 --custom_lr_milestones 0 30 60 80 --hpus 1 --max_train_batches 500 --epochs 5
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[0, 30, 60, 80], custom_lr_values=[0.1, 0.01, 0.001, 0.0001], data_path='/root/2114', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=1, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Traceback (most recent call last):
  File "resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 934, in _run
    call._call_setup_hook(self)  # allow user to setup lightning_module in accelerator environment
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 83, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 164, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/datamodule/datamodule.py", line 166, in setup
    self.dataset_train, self.dataset_val = load_data(
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/datamodule/datamodule.py", line 98, in load_data
    raise ValueError("Habana dataloader only supports Imagenet dataset")
ValueError: Habana dataloader only supports Imagenet dataset
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# mkdir -p data/pytorch/datasets/imagenet/ILSVRC2012/
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# cd data/pytorch/datasets/imagenet/ILSVRC2012/
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data/pytorch/datasets/imagenet/ILSVRC2012# mv ~/2114/train/ ./
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data/pytorch/datasets/imagenet/ILSVRC2012# mv ~/2114/val/ ./
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data/pytorch/datasets/imagenet/ILSVRC2012# cd ../
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data/pytorch/datasets/imagenet# cd ../
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data/pytorch/datasets# cd ../
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data/pytorch# cd ../
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data# ls
pytorch
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/data# cd ../
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# ls
data            lightning_logs  README.md         resnet50_PTL.py
datamodules.py  __pycache__     requirements.txt
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.1 0.01 0.001 0.0001 --custom_lr_milestones 0 30 60 80 --hpus 1 --max_train_batches 500 --epochs 5
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[0, 30, 60, 80], custom_lr_values=[0.1, 0.01, 0.001, 0.0001], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=1, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported

  | Name          | Type      | Params
--------------------------------------------
0 | model         | ResNet    | 25.6 M
1 | eval_best_acc | MaxMetric | 0     
--------------------------------------------
25.6 M    Trainable params
0         Non-trainable params
25.6 M    Total params
102.228   Total estimated model params size (MB)
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
Finding classes ... Done!
Done!
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 1 slice_index 0
random seed used  631274041
sliced media files/labels 1281167
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03447721/n03447721_43129.JPEG
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
Finding classes ... Done!
Done!
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 1 slice_index 0
random seed used  735741709
sliced media files/labels 50000
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n02130308/ILSVRC2012_val_00033687.JPEG
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Epoch 0:   0%|                                          | 0/500 [00:00<?, ?it/s]Shuffling ... Done!
Epoch 0: 100%|█| 500/500 [01:10<00:00,  7.10it/s, v_num=2, train_acc1=0.000, traINFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:1 is 0.1  
Epoch 1:   0%| | 0/500 [00:00<?, ?it/s, v_num=2, train_acc1=0.000, train_acc5=0.Shuffling ... Done!
Epoch 1: 100%|█| 500/500 [01:06<00:00,  7.51it/s, v_num=2, train_acc1=0.000, traINFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:2 is 0.1  
Epoch 2:   0%| | 0/500 [00:00<?, ?it/s, v_num=2, train_acc1=0.000, train_acc5=2.Shuffling ... Done!
Epoch 2: 100%|█| 500/500 [00:58<00:00,  8.56it/s, v_num=2, train_acc1=1.170, traINFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:3 is 0.1  
Epoch 3:   0%| | 0/500 [00:00<?, ?it/s, v_num=2, train_acc1=1.170, train_acc5=6.Shuffling ... Done!
Epoch 3: 100%|█| 500/500 [00:54<00:00,  9.23it/s, v_num=2, train_acc1=3.120, traINFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:4 is 0.1  
Epoch 4:   0%| | 0/500 [00:00<?, ?it/s, v_num=2, train_acc1=3.120, train_acc5=15Shuffling ... Done!
Epoch 4: 100%|█| 500/500 [00:53<00:00,  9.29it/s, v_num=2, train_acc1=4.690, traINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4: 100%|█| 500/500 [00:54<00:00,  9.18it/s, v_num=2, train_acc1=4.690, tra
Total Training time 326.32
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.1 0.01 0.001 0.0001 --custom_lr_milestones 0 30 60 80 --hpus 1 --max_train_batches 500 --epochs 5 --benchmark
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[0, 30, 60, 80], custom_lr_values=[0.1, 0.01, 0.001, 0.0001], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=1, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
Finding classes ... Done!
Done!
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 1 slice_index 0
random seed used  768201041
sliced media files/labels 1281167
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03447721/n03447721_43129.JPEG
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 3385.1604658672027
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 3426.675381058246
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 3577.635534360712
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 3451.276939870264
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 3619.0392405371613
INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.
INFO:pytorch_lightning.utilities.rank_zero: Best Epoch Time: 31.83
INFO:pytorch_lightning.utilities.rank_zero: Avg Epoch Time: 32.76
Total Training time 224.35
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.275 0.45 0.625 0.8 0.08 0.008 0.0008 --custom_lr_milestones 1 2 3 4 30 60 80 --hpus 8 --max_train_batches 500 --epochs 5
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.275, 0.45, 0.625, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 0] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=False, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
[rank: 5] Global seed set to 1234
[rank: 4] Global seed set to 1234
[rank: 7] Global seed set to 1234
[rank: 1] Global seed set to 1234
[rank: 2] Global seed set to 1234
[rank: 6] Global seed set to 1234
[rank: 3] Global seed set to 1234
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 5] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 7] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 1] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 2] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 4] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 6] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 3] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------
distributed_backend=hccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported

  | Name          | Type      | Params
--------------------------------------------
0 | model         | ResNet    | 25.6 M
1 | eval_best_acc | MaxMetric | 0     
--------------------------------------------
25.6 M    Trainable params
0         Non-trainable params
25.6 M    Total params
102.228   Total estimated model params size (MB)
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 7
random seed used  2048980637
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 2
random seed used  2049805746
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 5
random seed used  2050491014
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 6
random seed used  2049730958
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 0
random seed used  2049349353
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 3
random seed used  2047086028
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 1
random seed used  2044807234
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n07717556/n07717556_14716.JPEG
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
Running with Media API
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 4
random seed used  2048255180
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03717622/n03717622_12175.JPEG
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03447721/n03447721_43129.JPEG
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n06596364/n06596364_9591.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n02490219/n02490219_11648.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03126707/n03126707_6212.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03467068/n03467068_12171.JPEG
Running with Media API
Running with Media API
Running with Media API
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n07717556/n07717556_16229.JPEG
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
Finding classes ... Done!
Finding classes ... Done!
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:2
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 0
random seed used  1829232086
sliced media files/labels 6250
Finding largest file ...
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 1
random seed used  1831897530
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n01630670/ILSVRC2012_val_00046430.JPEG
sliced media files/labels 6250
Finding largest file ...
Running with Media API
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 2
random seed used  1845093957
sliced media files/labels 6250
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n04258138/ILSVRC2012_val_00039884.JPEG
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n02504013/ILSVRC2012_val_00017867.JPEG
Running with Media API
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 5
random seed used  1847672206
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 4
random seed used  1849692153
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 6
random seed used  1850064197
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 3
random seed used  1850012149
Generating labels ... Done!
Total media files/labels 50000 classes 1000
num_slices 8 slice_index 7
random seed used  1849148325
sliced media files/labels 6250
Finding largest file ...
sliced media files/labels 6250
Finding largest file ...
sliced media files/labels 6250
Finding largest file ...
sliced media files/labels 6250
Finding largest file ...
sliced media files/labels 6250
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n02927161/ILSVRC2012_val_00012569.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n02130308/ILSVRC2012_val_00041296.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n02130308/ILSVRC2012_val_00033687.JPEG
Running with Media API
Running with Media API
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n01983481/ILSVRC2012_val_00028475.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/val/n04258138/ILSVRC2012_val_00014066.JPEG
Running with Media API
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
INFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Epoch 0:   0%|                                          | 0/500 [00:00<?, ?it/s]Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Epoch 0: 100%|█| 500/500 [01:21<00:00,  6.13it/s, v_num=3, train_acc1=5.080, trINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Epoch 0: 100%|█| 500/500 [01:29<00:00,  5.56it/s, v_num=3, train_acc1=5.080, traINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:1 is 0.1
Epoch 1:   0%| | 0/500 [00:00<?, ?it/s, v_num=3, train_acc1=5.080, train_acc5=17Shuffling ... Done!
Epoch 1: 100%|█| 500/500 [00:59<00:00,  8.39it/s, v_num=3, train_acc1=20.70, trINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Epoch 1: 100%|█| 500/500 [01:07<00:00,  7.35it/s, v_num=3, train_acc1=20.70, traINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:2 is 0.275
Epoch 2:   0%| | 0/500 [00:00<?, ?it/s, v_num=3, train_acc1=20.70, train_acc5=38Shuffling ... Done!
Epoch 2:  88%|▉| 439/500 [00:44<00:06,  9.82it/s, v_num=3, train_acc1=31.60, train_acc5=53.90, val_acc1=14.80, val_acc5=34.30, epoch_eval_accuracy=14.90, top_eval_accEpoch 2:  88%|▉| 439/500 [00:44<00:06,  9.82it/s, v_num=3, train_acc1=37.10, train_acc5=60.20, val_acc1=14.80, val_acc5=34.30, epoch_eval_accuracy=14.90, top_eval_accEpoch 2: 100%|█| 500/500 [00:50<00:00,  9.81it/s, v_num=3, train_acc1=27.30, train_acc5=55.90, val_acc1=14.80, val_acc5=34.30, epoch_eval_accuracy=14.90, top_eval_acINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float████████████████████████████████████████████| 25/25 [00:00<00:00, 32.41it/s]
Epoch 2: 100%|█| 500/500 [00:51<00:00,  9.63it/s, v_num=3, train_acc1=27.30, train_acc5=55.90, val_acc1=27.30, val_acc5=52.30, epoch_eval_accuracy=27.50, top_eval_accINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float                                                                           
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:3 is 0.45
Epoch 3:   0%| | 0/500 [00:00<?, ?it/s, v_num=3, train_acc1=27.30, train_acc5=55.90, val_acc1=27.30, val_acc5=52.30, epoch_eval_accuracy=27.50, top_eval_accuracy=28.2Shuffling ... Done!
Epoch 3:  23%|▏| 114/500 [00:11<00:39,  9.76it/s, v_num=3, train_acc1=28.90, train_acc5=58.20, val_acc1=27.30, val_acc5=52.30, epoch_eval_accuracy=27.50, top_evEpoch 3:  23%|▏| 115/500 [00:11<00:39,  9.77it/s, v_num=3, train_acc1=28.90, train_acc5=58.20, val_acc1=27.30, val_acc5=52.30, epoch_eval_accuracy=27.50, top_evEpoch 3:  23%|▏| 115/500 [00:11<00:39,  9.75it/s, v_num=3, train_acc1=29.30, train_acc5=55.90, val_acc1=27.30, val_acc5=52.30, epoch_eval_accuracy=27.50, top_evEpoch 3:  23%|▏| 116/500 [00:11<00:39,  9.77it/s, v_num=3, train_acc1=29.30, tra                                                                                Epoch 3: 100%|█| 500/500 [00:50<00:00,  9.82it/s, v_num=3, train_acc1=41.40, trINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Epoch 3: 100%|█| 500/500 [00:51<00:00,  9.66it/s, v_num=3, train_acc1=41.40, traINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:The learning rate on epoch:4 is 0.625
Epoch 4:   0%| | 0/500 [00:00<?, ?it/s, v_num=3, train_acc1=41.40, train_acc5=64Shuffling ... Done!
Epoch 4: 100%|█| 500/500 [00:51<00:00,  9.65it/s, v_num=3, train_acc1=37.50, trINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Epoch 4: 100%|█| 500/500 [00:52<00:00,  9.49it/s, v_num=3, train_acc1=37.50, traINFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Total Training time 342.72
Total Training time 342.63
Total Training time 342.64
Total Training time 342.19
Total Training time 342.79
Total Training time 342.95
Total Training time 342.99
INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4: 100%|█| 500/500 [00:57<00:00,  8.62it/s, v_num=3, train_acc1=37.50, tra
Total Training time 351.83
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.275 0.45 0.625 0.8 0.08 0.008 0.0008 --custom_lr_milestones 1 2 3 4 30 60 80 --hpus 8 --max_train_batches 500 --epochs 5 --benchmark
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.275, 0.45, 0.625, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 0] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=False, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
[rank: 2] Global seed set to 1234
[rank: 5] Global seed set to 1234
[rank: 1] Global seed set to 1234
[rank: 3] Global seed set to 1234
[rank: 7] Global seed set to 1234
[rank: 4] Global seed set to 1234
[rank: 6] Global seed set to 1234
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 2] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 5] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 1] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 3] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 7] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 4] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
[rank: 6] Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------
distributed_backend=hccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
Finding classes ... Done!
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 4
random seed used  680301008
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 0
random seed used  682312467
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 1
random seed used  684588517
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 5
random seed used  669776196
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 7
random seed used  674981736
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n07717556/n07717556_16229.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n02490219/n02490219_11648.JPEG
Running with Media API
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 6
random seed used  682799031
Running with Media API
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 3
random seed used  676096430
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 2
random seed used  675504533
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03126707/n03126707_6212.JPEG
Running with Media API
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03447721/n03447721_43129.JPEG
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n07717556/n07717556_14716.JPEG
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n06596364/n06596364_9591.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03717622/n03717622_12175.JPEG
Running with Media API
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03467068/n03467068_12171.JPEG
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 27816.110001776415
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 27514.73684226261
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 27741.400058514668
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 27463.896274621104
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero: throughput_train = 27773.495980992568
INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=5` reached.
INFO:pytorch_lightning.utilities.rank_zero: Best Epoch Time: 33.13
INFO:pytorch_lightning.utilities.rank_zero: Avg Epoch Time: 33.36
Total Training time 268.36
Total Training time 267.55
Total Training time 275.90
Total Training time 267.84
Total Training time 267.95
Total Training time 268.27
Total Training time 268.09
Total Training time 267.82
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# PT_HPU_ENABLE_EAGER_CACHE=1 PT_HPU_LAZY_MODE=0 python3 resnet50_PTL.py  --batch_size 256 --data_path data/pytorch/datasets/imagenet/ILSVRC2012/ --autocast --custom_lr_values 0.275 0.45 0.625 0.8 0.08 0.008 0.0008 --custom_lr_milestones 1 2 3 4 30 60 80 --hpus 8 --max_train_batches 500 --epochs 5 --benchmark --hpu_torch_compile
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: Global seed set to 1234
INFO:lightning.fabric.utilities.seed:Global seed set to 1234
INFO:pytorch_lightning.utilities.rank_zero:The predetermined LR scheduler is: [0.1, 0.275, 0.45, 0.625, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008] for all epochs.
INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False
INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO:pytorch_lightning.utilities.rank_zero:HPU available: True, using: 0 HPUs
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
INFO: [rank: 0] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 0] Global seed set to 1234
INFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 7] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 7] Global seed set to 1234
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 1] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 1] Global seed set to 1234
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 3] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 3] Global seed set to 1234
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 2] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 2] Global seed set to 1234
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 4] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 4] Global seed set to 1234
WARNING:root:Habana Mixed Precision (HMP) module is deprecated and will be removed in version 1.13.0.
Please use native PyTorch autocast for mixed precision training and inference
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 6] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 6] Global seed set to 1234
Namespace(batch_size=256, benchmark=True, check_val_every_n_epoch=1, custom_lr_milestones=[1, 2, 3, 4, 30, 60, 80], custom_lr_values=[0.275, 0.45, 0.625, 0.8, 0.08, 0.008, 0.0008], data_path='data/pytorch/datasets/imagenet/ILSVRC2012/', dl_type='HABANA', epochs=5, hpu_torch_compile=True, hpus=8, is_autocast=True, lr=0.1, lr_gamma=0.1, lr_step_size=30, max_train_batches=500, patience=30, print_freq=1, warmup=50, workers=8)
INFO: [rank: 5] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 5] Global seed set to 1234
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
INFO: [rank: 7] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 7] Global seed set to 1234
INFO: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
INFO: [rank: 1] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 1] Global seed set to 1234
INFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
INFO: [rank: 3] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 3] Global seed set to 1234
INFO: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
INFO: [rank: 2] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 2] Global seed set to 1234
INFO: [rank: 5] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 5] Global seed set to 1234
INFO: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
INFO: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 0
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056389528 KB
------------------------------------------------------------------------------
INFO: [rank: 6] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 6] Global seed set to 1234
INFO: [rank: 4] Global seed set to 1234
INFO:lightning.fabric.utilities.seed:[rank: 4] Global seed set to 1234
INFO: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
INFO: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------
distributed_backend=hccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Train Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
Val Transforms supported
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name ResnetMediaPipe:1
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Finding classes ... Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Done!
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 5
random seed used  107230101
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 3
random seed used  111334620
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 6
random seed used  111124675
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 1
random seed used  110510732
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 7
random seed used  107896684
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03447721/n03447721_43129.JPEG
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 0
random seed used  113569886
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03467068/n03467068_12171.JPEG
Running with Media API
Running with Media API
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 4
random seed used  108988922
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n06596364/n06596364_9591.JPEG
sliced media files/labels 160146
Finding largest file ...
Generating labels ... Done!
Total media files/labels 1281167 classes 1000
num_slices 8 slice_index 2
random seed used  110630826
Running with Media API
sliced media files/labels 160146
Finding largest file ...
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03126707/n03126707_6212.JPEG
sliced media files/labels 160146
Finding largest file ...
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n07717556/n07717556_14716.JPEG
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n02490219/n02490219_11648.JPEG
Running with Media API
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n07717556/n07717556_16229.JPEG
Running with Media API
largest file is  data/pytorch/datasets/imagenet/ILSVRC2012/train/n03717622/n03717622_12175.JPEG
Running with Media API
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:Long tensor unsupported on HPU, casting to float
Shuffling ... Done!
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
Shuffling ... Done!
INFO:pytorch_lightning.utilities.rank_zero:Train Epoch start
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Shuffling ... Done!
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/7/fx_graphs.log'
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/2/fx_graphs.log'
Traceback (most recent call last):
  File "resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/0/fx_graphs.log'
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/5/fx_graphs.log'
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/1/fx_graphs.log'
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/6/fx_graphs.log'
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/habana_logs/3/fx_graphs.log'
Traceback (most recent call last):
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 389, in <module>
    time_interval=train_model(args)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 342, in train_model
    trainer.fit(model, datamodule=data_module)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 41, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 91, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 973, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 201, in run
    self.advance()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/core/optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning_habana/pytorch/strategies/parallel.py", line 134, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 256, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/amp.py", line 73, in optimizer_step
    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/hpex/optimizers/FusedSGD.py", line 85, in step
    loss = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/plugins/precision/precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/strategies/ddp.py", line 328, in training_step
    return self.model(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1504, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1113, in _run_ddp_forward
    return module_to_run(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/overrides/base.py", line 90, in forward
    output = self._forward_module.training_step(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/root/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet/resnet50_PTL.py", line 205, in training_step
    def training_step(self, batch, batch_idx):
  File "/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "<eval_with_key>.9", line 5, in forward
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/recipe_compiler.py", line 42, in __call__
    dump_fx_graph(self._fx_module, self._recipe_id)
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 31, in dump_fx_graph
    logger = get_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 26, in get_fx_graph_logger
    init_fx_graph_logger()
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 83, in init_fx_graph_logger
    logger.addHandler(_create_file_handler(log_file_name=FX_GRAPHS_LOGGER))
  File "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/dynamo/compile_backend/logger.py", line 57, in _create_file_handler
    file_handler = logging.FileHandler(_get_log_path(log_file_name))
  File "/usr/lib/python3.8/logging/__init__.py", line 1147, in __init__
    StreamHandler.__init__(self, self._open())
  File "/usr/lib/python3.8/logging/__init__.py", line 1176, in _open
    return open(self.baseFilename, self.mode, encoding=self.encoding)
root@sdsc-gaudi2:~/software/habana/Model-References/PyTorch/computer_vision/classification/lightning/resnet# 
```
