# Getting Started with ARA

Reservation leases are sensitive to time when scheduling the resources. Set your local timezone to simplify requests.

https://arawireless.readthedocs.io/en/latest/getting_started/started_with_ara_portal.html#setting-the-user-time-zone

First lease created successfully.

```
Lease Detail
Lease
Lease Name
icicle_hello_world_0
Lease ID
2d5b8ae0-8386-4656-93d8-24be62a95e9f
Project Id
3f7f6b770db14588b2a0f4d2ca98ef17
Start date
2024-03-11 22:55 EDT
End date
2024-03-11 23:55 EDT
Status
ACTIVE
Degraded
No
Reservations
Reservation ID
f9d4cc42-2249-4b17-b499-14b5f8a7a494
Status
active
Site
AgronomyFarm
Resource Type
ComputeNode
Device Type
Edge
Device ID
000
```

This might be a problem: *Please note that ARA portal allows only containers from arawirelesshub DockerHub repository.*

It seems like there may currently be no OpenStack API access to ARA. i.e., it is all Horizon/GUI-driven for provisioning. 

The following note about leases appears to be incorrect: *If you leave the Start Date and End Date blank, the default lease time is considered as 2 hours.* Observe explicit start and end times required to create lease.

Similarly, devices IDs appear to need to be explicitly requested as well, while the documentation states: *If the field is kept blank, any free node satisfying the Site, Resource Type, and Device Type attributes is reserved at random.*

Does this container-based deployment approach allow injection of cloud-init scripts? https://cloudinit.readthedocs.io/en/latest/

Are the default ssh-keygen settings sufficient these days? https://arawireless.readthedocs.io/en/latest/getting_started/ara_portal_extras.html#ara-jumpbox
Also might want to change how commands are displayed? i.e., do not start with a #

Is there any limit to container size?

Does the console always lockup when attempting to use vim? Is this an issue with its caching its swap files?

Is there somewhere we can find a more detailed network map?

Can multiple containers be assigned to the same compute nodes? RAN equipment? etc? Or are they exclusive? 

Installed same driver version and cuda version as used on Expanse. However, mismatch observed. Need to get host driver version installed on GPU nodes. Looks like GPU pass through may be possible though?
```
root@icicle_hello_world_host:/usr/bin# ./nvidia-smi 
Failed to initialize NVML: Driver/library version mismatch
root@icicle_hello_world_host:/usr/bin#
```

Gathering CPU info ...

```
root@icicle_hello_world_host:/tmp# lscpu
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         46 bits physical, 57 bits virtual
  Byte Order:            Little Endian
CPU(s):                  48
  On-line CPU(s) list:   0-47
Vendor ID:               GenuineIntel
  Model name:            Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz
    CPU family:          6
    Model:               106
    Thread(s) per core:  2
    Core(s) per socket:  12
    Socket(s):           2
    Stepping:            6
    BogoMIPS:            6000.00
    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_t
                         sc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr 
                         pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ss
                         bd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512
                         dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm
                         _mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md
                         _clear pconfig flush_l1d arch_capabilities
Virtualization features: 
  Virtualization:        VT-x
Caches (sum of all):     
  L1d:                   1.1 MiB (24 instances)
  L1i:                   768 KiB (24 instances)
  L2:                    30 MiB (24 instances)
  L3:                    36 MiB (2 instances)
NUMA:                    
  NUMA node(s):          2
  NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46
  NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47
Vulnerabilities:         
  Gather data sampling:  Mitigation; Microcode
  Itlb multihit:         Not affected
  L1tf:                  Not affected
  Mds:                   Not affected
  Meltdown:              Not affected
  Mmio stale data:       Mitigation; Clear CPU buffers; SMT vulnerable
  Retbleed:              Not affected
  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp
  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization
  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
  Srbds:                 Not affected
  Tsx async abort:       Not affected
root@icicle_hello_world_host:/tmp#
```


