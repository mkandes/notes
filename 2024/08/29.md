# SCC24: HOWTO run MPI applications on Expanse manually `--without-slurm` support

The primary issue you'll face if and when you attempt to replicate launching MPI applications on Expanse manually without builtin scheduler support is that most MPI distributions are quite good at auto-discovering the presence of many advanced features they can take advantage on your HPC system when you build and install them, even when you build them from source. This includes auto-discovery of a batch job schdeuler like Slurm available on Expanse. In fact, you actually have to explicitly disable Slurm support in OpenMPI with the `--without-slurm` flag during the build process to set OpenMPI in a state to learn how to use `mpirun` manually.

The latest versions of OpenMPI require UCX. As such, you first need to install a copy of UCX that is compatible with the version of OpenMPI you will install. Here is a build job script that I used to compile UCX v1.14.1.

```
#!/usr/bin/env bash

#SBATCH --job-name=ucx-1.14.1-make
#SBATCH --account=use300
#SBATCH --clusters=expanse
#SBATCH --partition=debug
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=00:30:00
#SBATCH --output=%x.o%j.%A.%a.%N
#SBATCH --array=0

declare -xir UNIX_TIME="$(date +'%s')"
declare -xr LOCAL_TIME="$(date +'%Y%m%dT%H%M%S%z')"

declare -xr LOCAL_SCRATCH="/scratch/${USER}/job_${SLURM_JOB_ID}"

declare -xr JOB_SCRIPT="$(scontrol show job ${SLURM_JOB_ID} | awk -F= '/Command=/{print $2}')"
declare -xr JOB_SCRIPT_MD5="$(md5sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_SHA256="$(sha256sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_NUMBER_OF_LINES="$(wc -l ${JOB_SCRIPT} | awk '{print $1}')"

declare -xr COMPILER_MODULE='gcc/8.5.0' # Default gcc version on Expanse from Rocky Linux 8.9

declare -xr UCX_MAJOR='1'
declare -xr UCX_MINOR='14'
declare -xr UCX_REV='1'
declare -xr UCX_VERSION="${UCX_MAJOR}.${UCX_MINOR}.${UCX_REV}"
declare -xr UCX_BUILD='default'
declare -xr UCX_ROOT_DIR="${HOME}/software/ucx"
declare -xr UCX_INSTALL_DIR="${UCX_ROOT_DIR}/${UCX_VERSION}/${UCX_BUILD}/${COMPILER_MODULE}"
declare -xr UCX_ROOT_URL='https://openucx.org'
declare -xr UCX_REPO_URL='https://github.com/openucx/ucx.git'

module purge
printenv

mkdir -p "${UCX_INSTALL_DIR}"
cd "${UCX_INSTALL_DIR}"

git clone "${UCX_REPO_URL}"
cd ucx
git checkout "v${UCX_VERSION}"
./autogen.sh
./contrib/configure-release --prefix="${UCX_INSTALL_DIR}"
make -j "${SLURM_CPUS_PER_TAKS}"
make install
```

You can submit the job to Slurm to compile and install UCX for you. This will not affect the Slurm integration problem. Once UCX is installed, you can then compile OpenMPI v4.1.6 against it. Here is a build job script for OpenMPI that utilizes the copy of UCX you've just compiled and installed. 

```
#!/usr/bin/env bash

#SBATCH --job-name=openmpi-4.1.6-make
#SBATCH --account=use300
#SBATCH --clusters=expanse
#SBATCH --partition=ind-shared
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=01:00:00
#SBATCH --output=%x.o%j.%A.%a.%N
#SBATCH --array=0

declare -xir UNIX_TIME="$(date +'%s')"
declare -xr LOCAL_TIME="$(date +'%Y%m%dT%H%M%S%z')"

declare -xr LOCAL_SCRATCH="/scratch/${USER}/job_${SLURM_JOB_ID}"

declare -xr JOB_SCRIPT="$(scontrol show job ${SLURM_JOB_ID} | awk -F= '/Command=/{print $2}')"
declare -xr JOB_SCRIPT_MD5="$(md5sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_SHA256="$(sha256sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_NUMBER_OF_LINES="$(wc -l ${JOB_SCRIPT} | awk '{print $1}')"

declare -xr COMPILER_MODULE='gcc/8.5.0' # Default gcc version on Expanse from Rocky Linux 8.9

declare -xr UCX_MAJOR='1'
declare -xr UCX_MINOR='14'
declare -xr UCX_REV='1'
declare -xr UCX_VERSION="${UCX_MAJOR}.${UCX_MINOR}.${UCX_REV}"
declare -xr UCX_BUILD='default'
declare -xr UCX_ROOT_DIR="${HOME}/software/ucx"
declare -xr UCX_INSTALL_DIR="${UCX_ROOT_DIR}/${UCX_VERSION}/${UCX_BUILD}/${COMPILER_MODULE}"

declare -xr OPENMPI_MAJOR='4'
declare -xr OPENMPI_MINOR='1'
declare -xr OPENMPI_REV='6'
declare -xr OPENMPI_VERSION="${OPENMPI_MAJOR}.${OPENMPI_MINOR}.${OPENMPI_REV}"
declare -xr OPENMPI_BUILD='default'
declare -xr OPENMPI_ROOT_DIR="${HOME}/software/openmpi"
declare -xr OPENMPI_INSTALL_DIR="${OPENMPI_ROOT_DIR}/${OPENMPI_VERSION}/${OPENMPI_BUILD}/${COMPILER_MODULE}"
declare -xr OPENMPI_ROOT_URL='https://www.open-mpi.org'
declare -xr OPENMPI_REPO_URL='https://github.com/open-mpi/ompi.git'

module purge
export PATH="${UCX_INSTALL_DIR}/bin:${PATH}"
export LD_LIBRARY_PATH="${UCX_INSTALL_DIR}/lib:${LD_LIBRARY_PATH}"
printenv

mkdir -p "${OPENMPI_INSTALL_DIR}"
cd "${OPENMPI_INSTALL_DIR}"

wget "https://download.open-mpi.org/release/open-mpi/v${OPENMPI_MAJOR}.${OPENMPI_MINOR}/openmpi-${OPENMPI_VERSION}.tar.gz"
tar -xf "openmpi-${OPENMPI_VERSION}.tar.gz"
cd "openmpi-${OPENMPI_VERSION}"
./configure --prefix="${OPENMPI_INSTALL_DIR}" \
  --enable-shared \
  --disable-silent-rules \
  --disable-builtin-atomics \
  --enable-static \
  --without-hcoll \
  --without-psm \
  --without-knem \
  --without-psm2 \
  --without-verbs \
  --without-ofi \
  --without-mxm \
  --without-fca \
  --without-xpmem \
  --with-ucx="${UCX_INSTALL_DIR}" \
  --without-ucc \
  --with-cma \
  --without-slurm \
  --without-tm \
  --without-loadleveler \
  --without-alps \
  --without-lsf \
  --without-sge \
  --disable-memchecker \
  --disable-java \
  --disable-mpi-java \
  --without-cuda \
  --disable-mpi-cxx \
  --disable-cxx-exceptions \
  --without-pmi \
  --with-pmix
make -j "${SLURM_CPUS_PER_TASK}"
make install
```



You can use the `salloc` command on a Slurm cluster to obtain a Slurm job allocation (i.e., a set of nodes and/or other compute resources). In the following example, I request access to a single CPU-core on two separate compute nodes available in Expanse's debug partition to test a simple MPI job, which we will run manually --- first with OpenMPI. 

```
[mkandes@login01 ~]$ salloc --account=use300 --partition=debug --nodes=2 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --time=00:30:00
salloc: Pending job allocation 33535802
salloc: job 33535802 queued and waiting for resources
salloc: job 33535802 has been allocated resources
salloc: Granted job allocation 33535802
salloc: Waiting for resource configuration
salloc: Nodes exp-9-[55-56] are ready for job
[mkandes@login01 ~]$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
        33102479_8   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_7   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_6   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_5   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_4   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_3   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail, Reserved for maintenance)
        33102479_1   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_0   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
   33534630_[4-19]   compute xhpl-WR1  mkandes PD       0:00      1 (Priority)
        33102479_9   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
          33535802     debug interact  mkandes  R       0:05      2 exp-9-[55-56]
[mkandes@login01 ~]$
```

```
[mkandes@login01 ~]$ sacct -j 33535802 --long
JobID        JobIDRaw        JobName  Partition  MaxVMSize  MaxVMSizeNode  MaxVMSizeTask  AveVMSize     MaxRSS MaxRSSNode MaxRSSTask     AveRSS MaxPages MaxPagesNode   MaxPagesTask   AvePages     MinCPU MinCPUNode MinCPUTask     AveCPU   NTasks  AllocCPUS    Elapsed      State ExitCode AveCPUFreq ReqCPUFreqMin ReqCPUFreqMax ReqCPUFreqGov     ReqMem ConsumedEnergy  MaxDiskRead MaxDiskReadNode MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteNode MaxDiskWriteTask   AveDiskWrite    ReqTRES  AllocTRES TRESUsageInAve TRESUsageInMax TRESUsageInMaxNode TRESUsageInMaxTask TRESUsageInMin TRESUsageInMinNode TRESUsageInMinTask TRESUsageInTot TRESUsageOutMax TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutAve TRESUsageOutTot 
------------ ------------ ---------- ---------- ---------- -------------- -------------- ---------- ---------- ---------- ---------- ---------- -------- ------------ -------------- ---------- ---------- ---------- ---------- ---------- -------- ---------- ---------- ---------- -------- ---------- ------------- ------------- ------------- ---------- -------------- ------------ --------------- --------------- -------------- ------------ ---------------- ---------------- -------------- ---------- ---------- -------------- -------------- ------------------ ------------------ -------------- ------------------ ------------------ -------------- --------------- ------------------- ------------------- --------------- --------------- 
33535802     33535802     interacti+      debug                                                                                                                                                                                                               2   00:01:05    RUNNING      0:0                  Unknown       Unknown       Unknown        16G                                                                                                                                          billing=2+ billing=2+                                                                                                                                                                                                                                 
33535802.ex+ 33535802.ex+     extern                                                                                                                                                                                                               2          2   00:01:05    RUNNING      0:0          0             0             0             0                         0                                                                                                                                      billing=2+                                                                                                                                                                                                                                 
[mkandes@login01 ~]$
```

Once the resources are allocated, you should be able to `ssh` into any compute node where you have a job allocation. You should also be able to `ssh` between the nodes directly, which will be important in manually launching your MPI applications across the nodes. 

```
[mkandes@login01 ~]$ ssh exp-9-55
Last login: Thu Aug 29 18:25:02 2024 from 10.21.0.20
[mkandes@exp-9-55 ~]$ squeue -p debug
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          33535646     debug     bash   jis038  R      17:45      1 exp-9-55
          33535802     debug interact  mkandes  R       6:04      2 exp-9-[55-56]
[mkandes@exp-9-55 ~]$ ssh exp-9-56
Last login: Thu Aug 29 18:22:10 2024 from 10.21.0.20
[mkandes@exp-9-56 ~]$
```


### References:
- https://slurm.schedmd.com/salloc.html
- https://docs.open-mpi.org/en/v5.0.x/launching-apps/ssh.html
- https://www.open-mpi.org/faq/?category=running#mpirun-hostfile
- https://www.open-mpi.org/faq/?category=running#mpirun-host
