# SCC24: How to run MPI applications manually on Expanse

### salloc

You can use the `salloc` command on a Slurm cluster to obtain a Slurm job allocation (i.e., a set of nodes and/or other compute resources). In the following example, I request access to a single CPU-core on two separate compute nodes available in Expanse's debug partition to test a simple MPI job, which we will run manually --- first with OpenMPI. 

```
[mkandes@login01 ~]$ salloc --account=use300 --partition=debug --nodes=2 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --time=00:30:00
salloc: Pending job allocation 33535802
salloc: job 33535802 queued and waiting for resources
salloc: job 33535802 has been allocated resources
salloc: Granted job allocation 33535802
salloc: Waiting for resource configuration
salloc: Nodes exp-9-[55-56] are ready for job
[mkandes@login01 ~]$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
        33102479_8   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_7   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_6   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_5   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_4   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_3   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail, Reserved for maintenance)
        33102479_1   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
        33102479_0   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
   33534630_[4-19]   compute xhpl-WR1  mkandes PD       0:00      1 (Priority)
        33102479_9   compute mlperf-t  mkandes PD       0:00      1 (ReqNodeNotAvail)
          33535802     debug interact  mkandes  R       0:05      2 exp-9-[55-56]
[mkandes@login01 ~]$
```

```
[mkandes@login01 ~]$ sacct -j 33535802 --long
JobID        JobIDRaw        JobName  Partition  MaxVMSize  MaxVMSizeNode  MaxVMSizeTask  AveVMSize     MaxRSS MaxRSSNode MaxRSSTask     AveRSS MaxPages MaxPagesNode   MaxPagesTask   AvePages     MinCPU MinCPUNode MinCPUTask     AveCPU   NTasks  AllocCPUS    Elapsed      State ExitCode AveCPUFreq ReqCPUFreqMin ReqCPUFreqMax ReqCPUFreqGov     ReqMem ConsumedEnergy  MaxDiskRead MaxDiskReadNode MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteNode MaxDiskWriteTask   AveDiskWrite    ReqTRES  AllocTRES TRESUsageInAve TRESUsageInMax TRESUsageInMaxNode TRESUsageInMaxTask TRESUsageInMin TRESUsageInMinNode TRESUsageInMinTask TRESUsageInTot TRESUsageOutMax TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutAve TRESUsageOutTot 
------------ ------------ ---------- ---------- ---------- -------------- -------------- ---------- ---------- ---------- ---------- ---------- -------- ------------ -------------- ---------- ---------- ---------- ---------- ---------- -------- ---------- ---------- ---------- -------- ---------- ------------- ------------- ------------- ---------- -------------- ------------ --------------- --------------- -------------- ------------ ---------------- ---------------- -------------- ---------- ---------- -------------- -------------- ------------------ ------------------ -------------- ------------------ ------------------ -------------- --------------- ------------------- ------------------- --------------- --------------- 
33535802     33535802     interacti+      debug                                                                                                                                                                                                               2   00:01:05    RUNNING      0:0                  Unknown       Unknown       Unknown        16G                                                                                                                                          billing=2+ billing=2+                                                                                                                                                                                                                                 
33535802.ex+ 33535802.ex+     extern                                                                                                                                                                                                               2          2   00:01:05    RUNNING      0:0          0             0             0             0                         0                                                                                                                                      billing=2+                                                                                                                                                                                                                                 
[mkandes@login01 ~]$
```

Once the resources are allocated, you should be able to `ssh` into any compute node where you have a job allocation. You should also be able to `ssh` between the nodes directly, which will be important in manually launching your MPI applications across the nodes. 

```
[mkandes@login01 ~]$ ssh exp-9-55
Last login: Thu Aug 29 18:25:02 2024 from 10.21.0.20
[mkandes@exp-9-55 ~]$ squeue -p debug
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          33535646     debug     bash   jis038  R      17:45      1 exp-9-55
          33535802     debug interact  mkandes  R       6:04      2 exp-9-[55-56]
[mkandes@exp-9-55 ~]$ ssh exp-9-56
Last login: Thu Aug 29 18:22:10 2024 from 10.21.0.20
[mkandes@exp-9-56 ~]$
```


### References:
- https://slurm.schedmd.com/salloc.html
- https://docs.open-mpi.org/en/v5.0.x/launching-apps/ssh.html
- https://www.open-mpi.org/faq/?category=running#mpirun-hostfile
- https://www.open-mpi.org/faq/?category=running#mpirun-host
