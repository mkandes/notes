# SCC24: HOWTO run MPI applications on Expanse manually `--without-slurm` support

The primary issue you'll face if and when you attempt to replicate launching MPI applications on Expanse manually without builtin scheduler support is that most MPI distributions are quite good at auto-discovering the presence of many advanced features they can take advantage on your HPC system when you build and install them, even when you build them from source. This includes auto-discovery of a batch job schdeuler like Slurm available on Expanse. In fact, you actually have to explicitly disable Slurm support in OpenMPI with the `--without-slurm` flag during the build process to set OpenMPI in a state to learn how to use `mpirun` manually.

The latest versions of OpenMPI require UCX. As such, you first need to install a copy of UCX that is compatible with the version of OpenMPI you will install. Here is a build job script that I used to compile UCX v1.14.1.

```
#!/usr/bin/env bash

#SBATCH --job-name=ucx-1.14.1-make
#SBATCH --account=use300
#SBATCH --clusters=expanse
#SBATCH --partition=debug
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=00:30:00
#SBATCH --output=%x.o%j.%A.%a.%N
#SBATCH --array=0

declare -xir UNIX_TIME="$(date +'%s')"
declare -xr LOCAL_TIME="$(date +'%Y%m%dT%H%M%S%z')"

declare -xr LOCAL_SCRATCH="/scratch/${USER}/job_${SLURM_JOB_ID}"

declare -xr JOB_SCRIPT="$(scontrol show job ${SLURM_JOB_ID} | awk -F= '/Command=/{print $2}')"
declare -xr JOB_SCRIPT_MD5="$(md5sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_SHA256="$(sha256sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_NUMBER_OF_LINES="$(wc -l ${JOB_SCRIPT} | awk '{print $1}')"

declare -xr COMPILER_MODULE='gcc/8.5.0' # Default gcc version on Expanse from Rocky Linux 8.9

declare -xr UCX_MAJOR='1'
declare -xr UCX_MINOR='14'
declare -xr UCX_REV='1'
declare -xr UCX_VERSION="${UCX_MAJOR}.${UCX_MINOR}.${UCX_REV}"
declare -xr UCX_BUILD='default'
declare -xr UCX_ROOT_DIR="${HOME}/software/ucx"
declare -xr UCX_INSTALL_DIR="${UCX_ROOT_DIR}/${UCX_VERSION}/${UCX_BUILD}/${COMPILER_MODULE}"
declare -xr UCX_ROOT_URL='https://openucx.org'
declare -xr UCX_REPO_URL='https://github.com/openucx/ucx.git'

module purge
printenv

mkdir -p "${UCX_INSTALL_DIR}"
cd "${UCX_INSTALL_DIR}"

git clone "${UCX_REPO_URL}"
cd ucx
git checkout "v${UCX_VERSION}"
./autogen.sh
./contrib/configure-release --prefix="${UCX_INSTALL_DIR}"
make -j "${SLURM_CPUS_PER_TAKS}"
make install
```

You can submit the job to Slurm with the `sbatch` command to compile and install UCX for you. This will not affect the Slurm integration problem. Once UCX is installed, you can then compile OpenMPI v4.1.6 against it. Here is a build job script for OpenMPI that utilizes the copy of UCX you've just compiled and installed. 

```
#!/usr/bin/env bash

#SBATCH --job-name=openmpi-4.1.6-make
#SBATCH --account=use300
#SBATCH --clusters=expanse
#SBATCH --partition=ind-shared
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=01:00:00
#SBATCH --output=%x.o%j.%A.%a.%N
#SBATCH --array=0

declare -xir UNIX_TIME="$(date +'%s')"
declare -xr LOCAL_TIME="$(date +'%Y%m%dT%H%M%S%z')"

declare -xr LOCAL_SCRATCH="/scratch/${USER}/job_${SLURM_JOB_ID}"

declare -xr JOB_SCRIPT="$(scontrol show job ${SLURM_JOB_ID} | awk -F= '/Command=/{print $2}')"
declare -xr JOB_SCRIPT_MD5="$(md5sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_SHA256="$(sha256sum ${JOB_SCRIPT} | awk '{print $1}')"
declare -xr JOB_SCRIPT_NUMBER_OF_LINES="$(wc -l ${JOB_SCRIPT} | awk '{print $1}')"

declare -xr COMPILER_MODULE='gcc/8.5.0' # Default gcc version on Expanse from Rocky Linux 8.9

declare -xr UCX_MAJOR='1'
declare -xr UCX_MINOR='14'
declare -xr UCX_REV='1'
declare -xr UCX_VERSION="${UCX_MAJOR}.${UCX_MINOR}.${UCX_REV}"
declare -xr UCX_BUILD='default'
declare -xr UCX_ROOT_DIR="${HOME}/software/ucx"
declare -xr UCX_INSTALL_DIR="${UCX_ROOT_DIR}/${UCX_VERSION}/${UCX_BUILD}/${COMPILER_MODULE}"

declare -xr OPENMPI_MAJOR='4'
declare -xr OPENMPI_MINOR='1'
declare -xr OPENMPI_REV='6'
declare -xr OPENMPI_VERSION="${OPENMPI_MAJOR}.${OPENMPI_MINOR}.${OPENMPI_REV}"
declare -xr OPENMPI_BUILD='default'
declare -xr OPENMPI_ROOT_DIR="${HOME}/software/openmpi"
declare -xr OPENMPI_INSTALL_DIR="${OPENMPI_ROOT_DIR}/${OPENMPI_VERSION}/${OPENMPI_BUILD}/${COMPILER_MODULE}"
declare -xr OPENMPI_ROOT_URL='https://www.open-mpi.org'
declare -xr OPENMPI_REPO_URL='https://github.com/open-mpi/ompi.git'

module purge
export PATH="${UCX_INSTALL_DIR}/bin:${PATH}"
export LD_LIBRARY_PATH="${UCX_INSTALL_DIR}/lib:${LD_LIBRARY_PATH}"
printenv

mkdir -p "${OPENMPI_INSTALL_DIR}"
cd "${OPENMPI_INSTALL_DIR}"

wget "https://download.open-mpi.org/release/open-mpi/v${OPENMPI_MAJOR}.${OPENMPI_MINOR}/openmpi-${OPENMPI_VERSION}.tar.gz"
tar -xf "openmpi-${OPENMPI_VERSION}.tar.gz"
cd "openmpi-${OPENMPI_VERSION}"
./configure --prefix="${OPENMPI_INSTALL_DIR}" \
  --enable-shared \
  --disable-silent-rules \
  --disable-builtin-atomics \
  --enable-static \
  --without-hcoll \
  --without-psm \
  --without-knem \
  --without-psm2 \
  --without-verbs \
  --without-ofi \
  --without-mxm \
  --without-fca \
  --without-xpmem \
  --with-ucx="${UCX_INSTALL_DIR}" \
  --without-ucc \
  --with-cma \
  --without-slurm \
  --without-tm \
  --without-loadleveler \
  --without-alps \
  --without-lsf \
  --without-sge \
  --disable-memchecker \
  --disable-java \
  --disable-mpi-java \
  --without-cuda \
  --disable-mpi-cxx \
  --disable-cxx-exceptions \
  --without-pmi \
  --with-pmix
make -j "${SLURM_CPUS_PER_TASK}"
make install
```

Again, you can submit this job script to Slurm. It will not affect the Slurm integration issue with the build itself. Note, however, the build job as configured will take approimately 40 minutes to compile and install. So, please be patient. Once done, however, you can use the `salloc` command on to obtain a Slurm job allocation (i.e., a set of nodes and/or other compute resources). In the following example, I request access to a single CPU-core on two separate compute nodes available in Expanse's debug partition to test a simple MPI job, which we will run manually with our newly installed, non-Slurm-integrated copy of OpenMPI. 

```
[mkandes@login02 ~]$ srun --account=use300 --partition=debug --nodes=2 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --time=00:30:00 --pty --wait=0 /bin/bash
srun: job 33628684 queued and waiting for resources
srun: job 33628684 has been allocated resources
[mkandes@exp-9-55 ~]$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          33628684     debug     bash  mkandes  R       0:06      2 exp-9-[55-56]
[mkandes@exp-9-55 ~]$
```

Once the resources are allocated, you should be able to `ssh` into any compute node where you have a job allocation. You should also be able to `ssh` between the nodes directly, which will be important in manually launching your MPI applications across the nodes. 

```
[mkandes@login02 ~]$ salloc --account=use300 --partition=debug --nodes=2 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --time=00:30:00
salloc: Pending job allocation 33628690
salloc: job 33628690 queued and waiting for resources
salloc: job 33628690 has been allocated resources
salloc: Granted job allocation 33628690
salloc: Waiting for resource configuration
salloc: Nodes exp-9-[55-56] are ready for job
[mkandes@login02 ~]$ squeue -u $USER
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          33628690     debug interact  mkandes  R       0:05      2 exp-9-[55-56]
[mkandes@login02 ~]$ ssh exp-9-55
Last login: Tue Sep  3 12:09:44 2024 from 10.21.0.20
[mkandes@exp-9-55 ~]$
```

After you've ssh'd into one of the nodes, you'll need to configure your software environment to access the OpenMPI distribution. Many of the lines that follow are simply taken from the build job scripts used to compile UCX and OpenMPI (with the default gcc compilers available from Rocky Linux 8.9 on Expanse). 

```
[mkandes@exp-9-55 ~]$ declare -xr COMPILER_MODULE='gcc/8.5.0'
[mkandes@exp-9-55 ~]$ declare -xr UCX_MAJOR='1'
[mkandes@exp-9-55 ~]$ declare -xr UCX_MINOR='14'
[mkandes@exp-9-55 ~]$ declare -xr UCX_REV='1'
[mkandes@exp-9-55 ~]$ declare -xr UCX_VERSION="${UCX_MAJOR}.${UCX_MINOR}.${UCX_REV}"
[mkandes@exp-9-55 ~]$ declare -xr UCX_BUILD='default'
[mkandes@exp-9-55 ~]$ declare -xr UCX_ROOT_DIR="${HOME}/software/ucx"
[mkandes@exp-9-55 ~]$ declare -xr UCX_INSTALL_DIR="${UCX_ROOT_DIR}/${UCX_VERSION}/${UCX_BUILD}/${COMPILER_MODULE}"
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_MAJOR='4'
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_MINOR='1'
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_REV='6'
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_VERSION="${OPENMPI_MAJOR}.${OPENMPI_MINOR}.${OPENMPI_REV}"
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_BUILD='default'
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_ROOT_DIR="${HOME}/software/openmpi"
[mkandes@exp-9-55 ~]$ declare -xr OPENMPI_INSTALL_DIR="${OPENMPI_ROOT_DIR}/${OPENMPI_VERSION}/${OPENMPI_BUILD}/${COMPILER_MODULE}"
[mkandes@exp-9-55 ~]$ module purge
[mkandes@exp-9-55 ~]$ export PATH="${UCX_INSTALL_DIR}/bin:${PATH}"
[mkandes@exp-9-55 ~]$ export LD_LIBRARY_PATH="${UCX_INSTALL_DIR}/lib:${LD_LIBRARY_PATH}"
[mkandes@exp-9-55 ~]$ export PATH="${OPENMPI_INSTALL_DIR}/bin:${PATH}"
[mkandes@exp-9-55 ~]$ export LD_LIBRARY_PATH="${OPENMPI_INSTALL_DIR}/lib:${LD_LIBRARY_PATH}"
[mkandes@exp-9-55 ~]$ which mpirun
~/software/openmpi/4.1.6/default/gcc/8.5.0/bin/mpirun
[mkandes@exp-9-55 ~]$ mpirun --version
mpirun (Open MPI) 4.1.6

Report bugs to http://www.open-mpi.org/community/help/
[mkandes@exp-9-55 ~]$
```

Now you can begin experimenting with running MPI applications manually! Let's simply start by trying to run the `hostname` command across our two CPUs on the two allocated nodes. 

```
[mkandes@exp-9-55 ~]$ mpirun -host exp-9-55,exp-9-56 -n 2 hostname -f
bash: orted: command not found
--------------------------------------------------------------------------
ORTE was unable to reliably start one or more daemons.
This usually is caused by:

* not finding the required libraries and/or binaries on
  one or more nodes. Please check your PATH and LD_LIBRARY_PATH
  settings, or configure OMPI with --enable-orterun-prefix-by-default

* lack of authority to execute on one or more specified nodes.
  Please verify your allocation and authorities.

* the inability to write startup files into /tmp (--tmpdir/orte_tmpdir_base).
  Please check with your sys admin to determine the correct location to use.

*  compilation of the orted with dynamic libraries when static are required
  (e.g., on Cray). Please check your configure cmd line and consider using
  one of the contrib/platform definitions for your system type.

* an inability to create a connection back to mpirun due to a
  lack of common network interfaces and/or no route found between
  them. Please check network connectivity (including firewalls
  and network routing requirements).
--------------------------------------------------------------------------
[mkandes@exp-9-55 ~]$
```

Oops! While you may have setup the software environment on `exp-9-55` correctly, how does does any process started on `exp-9-56` know where you installed OpenMPI? In this case, you actually only need to add the `--prefix` option. 

```
[mkandes@exp-9-55 ~]$ mpirun -host exp-9-55,exp-9-56 --prefix "${OPENMPI_INSTALL_DIR}" -n 2 hostname -f
exp-9-55.expanse.sdsc.edu
exp-9-56.expanse.sdsc.edu
```

It may be useful to compare the differences in the environments on each node, respectively.

```
[mkandes@exp-9-55 ~]$ mpirun -host exp-9-55 --prefix "${OPENMPI_INSTALL_DIR}" -n 1 printenv
OMPI_MCA_pmix=^s1,s2,cray,isolated
PMIX_MCA_mca_base_component_show_load_errors=1
OMPI_COMMAND=printenv
OMPI_MCA_orte_precondition_transports=75bbb13e958d723b-cda3315dfe48c55c
CONDA_SHLVL=0
LD_LIBRARY_PATH=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/lib:/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/lib:/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/lib:/home/mkandes/software/ucx/1.14.1/default/gcc/8.5.0/lib:
LS_COLORS=rs=0:di=38;5;33:ln=38;5;51:mh=00:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=01;05;37;41:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;40:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.zst=38;5;9:*.tzst=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.wim=38;5;9:*.swm=38;5;9:*.dwm=38;5;9:*.esd=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.mjpg=38;5;13:*.mjpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.m4a=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.oga=38;5;45:*.opus=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
CONDA_EXE=/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/bin/conda
__LMOD_REF_COUNT_PATH=/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/condabin:1;/home/mkandes/.local/bin:1;/home/mkandes/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1
_ModuleTable002_=ZHVsZWZpbGVzOi91c3Ivc2hhcmUvbW9kdWxlZmlsZXM6L3Vzci9zaGFyZS9Nb2R1bGVzL21vZHVsZWZpbGVzIix9
SSH_CONNECTION=10.21.0.20 47300 10.21.9.55 22
OPENMPI_BUILD=default
UCX_INSTALL_DIR=/home/mkandes/software/ucx/1.14.1/default/gcc/8.5.0
OPENMPI_MAJOR=4
LANG=en_US.UTF-8
HISTCONTROL=ignoredups
COMPILER_MODULE=gcc/8.5.0
HOSTNAME=exp-9-55
LMOD_SYSTEM_DEFAULT_MODULES=DefaultModules
UCX_BUILD=default
UCX_MAJOR=1
OPENMPI_MINOR=1
S_COLORS=auto
_CE_M=
which_declare=declare -f
UCX_VERSION=1.14.1
OPENMPI_ROOT_DIR=/home/mkandes/software/openmpi
UCX_MINOR=14
XDG_SESSION_ID=1424
USER=mkandes
__LMOD_REF_COUNT_MODULEPATH=/cm/local/modulefiles:1;/cm/shared/apps/access/modulefiles:1;/etc/modulefiles:1;/usr/share/modulefiles:1;/usr/share/Modules/modulefiles:1;/cm/shared/modulefiles:1
PWD=/home/mkandes
ENABLE_LMOD=1
HOME=/home/mkandes
CONDA_PYTHON_EXE=/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/bin/python
LMOD_COLORIZE=yes
LMOD_SYSHOST=expanse
SSH_CLIENT=10.21.0.20 47300 22
LMOD_VERSION=8.2.4
LMOD_SETTARG_CMD=:
OPENMPI_VERSION=4.1.6
BASH_ENV=/usr/share/lmod/lmod/init/bash
_CE_CONDA=
UCX_ROOT_DIR=/home/mkandes/software/ucx
OPENMPI_REV=6
LMOD_sys=Linux
_ModuleTable001_=X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXt9LG1wYXRoQT17Ii9jbS9sb2NhbC9tb2R1bGVmaWxlcyIsIi9jbS9zaGFyZWQvYXBwcy9hY2Nlc3MvbW9kdWxlZmlsZXMiLCIvZXRjL21vZHVsZWZpbGVzIiwiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcyIsIi91c3Ivc2hhcmUvTW9kdWxlcy9tb2R1bGVmaWxlcyIsIi9jbS9zaGFyZWQvbW9kdWxlZmlsZXMiLH0sWyJzeXN0ZW1CYXNlTVBBVEgiXT0iL2NtL2xvY2FsL21vZHVsZWZpbGVzOi9jbS9zaGFyZWQvYXBwcy9hY2Nlc3MvbW9kdWxlZmlsZXM6L2NtL3NoYXJlZC9tb2R1bGVmaWxlczovZXRjL21v
__LMOD_REF_COUNT_MANPATH=/cm/shared/apps/slurm/current/man:1;/usr/share/lmod/lmod/share/man:1;/usr/local/share/man:1;/usr/share/man:1;/cm/local/apps/environment-modules/current/share/man:1
_ModuleTable003_=TmFtZSJdPSJzaGFyZWQiLH0sc2x1cm09e1siZm4iXT0iL2NtL2xvY2FsL21vZHVsZWZpbGVzL3NsdXJtL2V4cGFuc2UvMjMuMDIuNyIsWyJmdWxsTmFtZSJdPSJzbHVybS9leHBhbnNlLzIzLjAyLjciLFsibG9hZE9yZGVyIl09Myxwcm9wVD17fSxbInN0YWNrRGVwdGgiXT0yLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJzbHVybSIsfSx9LG1wYXRoQT17Ii9jbS9zaGFyZWQvYXBwcy9zcGFjay8wLjE3LjMvY3B1L2Ivc2hhcmUvc3BhY2svbG1vZC9saW51eC1yb2NreTgteDg2XzY0L0NvcmUiLCIvY20vbG9jYWwvbW9kdWxlZmlsZXMiLCIvY20vc2hhcmVkL2FwcHMvYWNjZXNzL21vZHVsZWZpbGVzIiwiL2V0Yy9tb2R1bGVmaWxlcyIsIi91c3Ivc2hhcmUvbW9k
LMOD_ROOT=/usr/share/lmod
SSH_TTY=/dev/pts/0
MAIL=/var/spool/mail/mkandes
LMOD_arch=x86_64
__Init_Default_Modules=1
TERM=xterm-256color
SHELL=/bin/bash
_ModuleTable_Sz_=2
OPENMPI_INSTALL_DIR=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0
SHLVL=1
MANPATH=/cm/shared/apps/slurm/current/man:/usr/share/lmod/lmod/share/man::/usr/local/share/man:/usr/share/man:/cm/local/apps/environment-modules/current/share/man
UCX_REV=1
LMOD_PREPEND_BLOCK=normal
MODULEPATH=/cm/local/modulefiles:/cm/shared/apps/access/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/share/Modules/modulefiles:/cm/shared/modulefiles
LOGNAME=mkandes
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/501506/bus
XDG_RUNTIME_DIR=/run/user/501506
MODULEPATH_ROOT=/usr/share/modulefiles
LMOD_PACKAGE_PATH=/usr/share/lmod/etc/site/lmod
PATH=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin:/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin:/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin:/home/mkandes/software/ucx/1.14.1/default/gcc/8.5.0/bin:/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/condabin:/home/mkandes/.local/bin:/home/mkandes/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
DEBUGINFOD_URLS=https://debuginfod.centos.org/ 
MODULESHOME=/usr/share/lmod/lmod
LMOD_SETTARG_FULL_SUPPORT=no
HISTSIZE=-1
LMOD_PKG=/usr/share/lmod/lmod
LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod
LESSOPEN=||/usr/bin/lesspipe.sh %s
LMOD_FULL_SETTARG_SUPPORT=no
LMOD_DIR=/usr/share/lmod/lmod/libexec
BASH_FUNC_which%%=() {  ( alias;
 eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@
}
BASH_FUNC_module%%=() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)
}
BASH_FUNC_ml%%=() {  eval $($LMOD_DIR/ml_cmd "$@")
}
_=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin/mpirun
IPATH_NO_BACKTRACE=1
HFI_NO_BACKTRACE=1
OMPI_MCA_orte_local_daemon_uri=1974927360.0;tcp://10.21.9.55,10.22.9.55,198.202.101.232:39101
OMPI_MCA_orte_hnp_uri=1974927360.0;tcp://10.21.9.55,10.22.9.55,198.202.101.232:39101
OMPI_MCA_mpi_oversubscribe=0
OMPI_MCA_orte_app_num=0
OMPI_UNIVERSE_SIZE=1
OMPI_MCA_orte_num_nodes=1
OMPI_MCA_shmem_RUNTIME_QUERY_hint=mmap
OMPI_MCA_orte_bound_at_launch=1
OMPI_MCA_ess=^singleton
OMPI_MCA_orte_ess_num_procs=1
OMPI_COMM_WORLD_SIZE=1
OMPI_COMM_WORLD_LOCAL_SIZE=1
OMPI_MCA_orte_tmpdir_base=/tmp
OMPI_MCA_orte_top_session_dir=/tmp/ompi.exp-9-55.501506
OMPI_MCA_orte_jobfam_session_dir=/tmp/ompi.exp-9-55.501506/pid.432657
OMPI_NUM_APP_CTX=1
OMPI_FIRST_RANKS=0
OMPI_APP_CTX_NUM_PROCS=1
OMPI_MCA_initial_wdir=/home/mkandes
OMPI_MCA_orte_launch=1
PMIX_NAMESPACE=1974927361
PMIX_RANK=0
PMIX_SERVER_URI3=1974927360.0;tcp4://127.0.0.1:60133
PMIX_SERVER_URI2=1974927360.0;tcp4://127.0.0.1:60133
PMIX_SERVER_URI21=1974927360.0;tcp4://127.0.0.1:60133
PMIX_SECURITY_MODE=native
PMIX_PTL_MODULE=tcp,usock
PMIX_BFROP_BUFFER_TYPE=PMIX_BFROP_BUFFER_NON_DESC
PMIX_GDS_MODULE=ds21,ds12,hash
PMIX_SERVER_TMPDIR=/tmp/ompi.exp-9-55.501506/pid.432657
PMIX_SYSTEM_TMPDIR=/tmp
PMIX_DSTORE_21_BASE_PATH=/tmp/ompi.exp-9-55.501506/pid.432657/pmix_dstor_ds21_432657
PMIX_DSTORE_ESH_BASE_PATH=/tmp/ompi.exp-9-55.501506/pid.432657/pmix_dstor_ds12_432657
PMIX_HOSTNAME=exp-9-55
PMIX_VERSION=3.2.5a1
OMPI_MCA_ess_base_jobid=1974927361
OMPI_MCA_ess_base_vpid=0
OMPI_COMM_WORLD_RANK=0
OMPI_COMM_WORLD_LOCAL_RANK=0
OMPI_COMM_WORLD_NODE_RANK=0
OMPI_MCA_orte_ess_node_rank=0
PMIX_ID=1974927361.0
OMPI_FILE_LOCATION=/tmp/ompi.exp-9-55.501506/pid.432657/0/0
[mkandes@exp-9-55 ~]$
```

```
[mkandes@exp-9-55 ~]$ mpirun -host exp-9-56 --prefix "${OPENMPI_INSTALL_DIR}" -n 1 printenv
OMPI_MCA_pmix=^s1,s2,cray,isolated
PMIX_MCA_mca_base_component_show_load_errors=1
OMPI_COMMAND=printenv
OMPI_MCA_orte_precondition_transports=0bfefd9b40e60288-115dd403547861fe
CONDA_SHLVL=0
LD_LIBRARY_PATH=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/lib:/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/lib:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64
CONDA_EXE=/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/bin/conda
__LMOD_REF_COUNT_PATH=/cm/shared/apps/sdsc/1.0/bin:1;/cm/shared/apps/sdsc/1.0/sbin:1;/cm/shared/apps/slurm/current/sbin:1;/cm/shared/apps/slurm/current/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1
_ModuleTable002_=XT0yLHByb3BUPXthcmNoPXtbImNwdSJdPTEsfSx9LFsic3RhY2tEZXB0aCJdPTEsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09ImNwdSIsfSxzZHNjPXtbImZuIl09Ii9jbS9zaGFyZWQvbW9kdWxlZmlsZXMvc2RzYy8xLjAubHVhIixbImZ1bGxOYW1lIl09InNkc2MvMS4wIixbImxvYWRPcmRlciJdPTQscHJvcFQ9e30sWyJzdGFja0RlcHRoIl09MSxbInN0YXR1cyJdPSJhY3RpdmUiLFsidXNlck5hbWUiXT0ic2RzYyIsfSxzaGFyZWQ9e1siZm4iXT0iL2NtL2xvY2FsL21vZHVsZWZpbGVzL3NoYXJlZCIsWyJmdWxsTmFtZSJdPSJzaGFyZWQiLFsibG9hZE9yZGVyIl09MSxwcm9wVD17fSxbInN0YWNrRGVwdGgiXT0xLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2Vy
SSH_CONNECTION=10.21.9.55 51896 10.21.9.56 22
LANG=en_US.UTF-8
SDSC_BIN=/cm/shared/apps/sdsc/1.0/bin
LMOD_SYSTEM_DEFAULT_MODULES=DefaultModules
__LMOD_REF_COUNT__LMFILES_=/cm/local/modulefiles/shared:1;/usr/share/modulefiles/cpu/0.17.3b.lua:1;/cm/local/modulefiles/slurm/expanse/23.02.7:1;/cm/shared/modulefiles/sdsc/1.0.lua:1;/usr/share/modulefiles/DefaultModules.lua:1
SDSC_DIR=/cm/shared/apps/sdsc/1.0
DYLD_LIBRARY_PATH=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/lib:
__LMOD_REF_COUNT_LD_LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:1;/cm/shared/apps/slurm/current/lib64:1
_ModuleTable004_=dWxlZmlsZXMiLCIvdXNyL3NoYXJlL01vZHVsZXMvbW9kdWxlZmlsZXMiLCIvY20vc2hhcmVkL21vZHVsZWZpbGVzIix9LFsic3lzdGVtQmFzZU1QQVRIIl09Ii9jbS9sb2NhbC9tb2R1bGVmaWxlczovY20vc2hhcmVkL2FwcHMvYWNjZXNzL21vZHVsZWZpbGVzOi9jbS9zaGFyZWQvbW9kdWxlZmlsZXM6L2V0Yy9tb2R1bGVmaWxlczovdXNyL3NoYXJlL21vZHVsZWZpbGVzOi91c3Ivc2hhcmUvTW9kdWxlcy9tb2R1bGVmaWxlcyIsfQ==
S_COLORS=auto
_CE_M=
which_declare=declare -f
XDG_SESSION_ID=12348
USER=mkandes
__LMOD_REF_COUNT_MODULEPATH=/cm/shared/apps/spack/0.17.3/cpu/b/share/spack/lmod/linux-rocky8-x86_64/Core:1;/cm/local/modulefiles:1;/cm/shared/apps/access/modulefiles:1;/etc/modulefiles:1;/usr/share/modulefiles:1;/usr/share/Modules/modulefiles:1;/cm/shared/modulefiles:2
__LMOD_REF_COUNT_LOADEDMODULES=shared:1;cpu/0.17.3b:1;slurm/expanse/23.02.7:1;sdsc/1.0:1;DefaultModules:1
PWD=/home/mkandes
ENABLE_LMOD=1
HOME=/home/mkandes
CONDA_PYTHON_EXE=/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/bin/python
LMOD_COLORIZE=yes
LMOD_SYSHOST=expanse
SSH_CLIENT=10.21.9.55 51896 22
LMOD_VERSION=8.2.4
CPATH=/cm/shared/apps/slurm/current/include
LMOD_SETTARG_CMD=:
BASH_ENV=/usr/share/lmod/lmod/init/bash
SDSC_SPACK_STACK=cpu
_CE_CONDA=
__LMOD_REF_COUNT_LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:1;/cm/shared/apps/slurm/current/lib64:1
LIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64
LMOD_sys=Linux
_ModuleTable001_=X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXtEZWZhdWx0TW9kdWxlcz17WyJmbiJdPSIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0RlZmF1bHRNb2R1bGVzLmx1YSIsWyJmdWxsTmFtZSJdPSJEZWZhdWx0TW9kdWxlcyIsWyJsb2FkT3JkZXIiXT01LHByb3BUPXt9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09IkRlZmF1bHRNb2R1bGVzIix9LGNwdT17WyJmbiJdPSIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL2NwdS8wLjE3LjNiLmx1YSIsWyJmdWxsTmFtZSJdPSJjcHUvMC4xNy4zYiIsWyJsb2FkT3JkZXIi
SLURM_CONF=/cm/shared/apps/slurm/var/etc/expanse/slurm.conf
LOADEDMODULES=shared:cpu/0.17.3b:slurm/expanse/23.02.7:sdsc/1.0:DefaultModules
__LMOD_REF_COUNT_MANPATH=/cm/shared/apps/slurm/current/man:1;/usr/share/lmod/lmod/share/man:1
_ModuleTable003_=TmFtZSJdPSJzaGFyZWQiLH0sc2x1cm09e1siZm4iXT0iL2NtL2xvY2FsL21vZHVsZWZpbGVzL3NsdXJtL2V4cGFuc2UvMjMuMDIuNyIsWyJmdWxsTmFtZSJdPSJzbHVybS9leHBhbnNlLzIzLjAyLjciLFsibG9hZE9yZGVyIl09Myxwcm9wVD17fSxbInN0YWNrRGVwdGgiXT0yLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJzbHVybSIsfSx9LG1wYXRoQT17Ii9jbS9zaGFyZWQvYXBwcy9zcGFjay8wLjE3LjMvY3B1L2Ivc2hhcmUvc3BhY2svbG1vZC9saW51eC1yb2NreTgteDg2XzY0L0NvcmUiLCIvY20vbG9jYWwvbW9kdWxlZmlsZXMiLCIvY20vc2hhcmVkL2FwcHMvYWNjZXNzL21vZHVsZWZpbGVzIiwiL2V0Yy9tb2R1bGVmaWxlcyIsIi91c3Ivc2hhcmUvbW9k
LMOD_ROOT=/usr/share/lmod
LMOD_arch=x86_64
__Init_Default_Modules=1
CMD_WLM_CLUSTER_NAME=expanse
SHELL=/bin/bash
_ModuleTable_Sz_=4
__LMOD_REF_COUNT_CPATH=/cm/shared/apps/slurm/current/include:1
SDSC_LIB=/cm/shared/apps/sdsc/1.0/lib
SHLVL=1
SDSC_SBIN=/cm/shared/apps/sdsc/1.0/sbin
MANPATH=/cm/shared/apps/slurm/current/man:/usr/share/lmod/lmod/share/man::/cm/shared/apps/slurm/current/man:/usr/local/share/man:/usr/share/man:/cm/local/apps/environment-modules/current/share/man
LMOD_PREPEND_BLOCK=normal
MODULEPATH=/cm/shared/apps/spack/0.17.3/cpu/b/share/spack/lmod/linux-rocky8-x86_64/Core:/cm/local/modulefiles:/cm/shared/apps/access/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/share/Modules/modulefiles:/cm/shared/modulefiles
LOGNAME=mkandes
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/501506/bus
XDG_RUNTIME_DIR=/run/user/501506
MODULEPATH_ROOT=/usr/share/modulefiles
LMOD_PACKAGE_PATH=/usr/share/lmod/etc/site/lmod
PATH=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin:/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin:/home/mkandes/software/miniconda/Miniconda3-py312_24.1.2-0-Linux-x86_64/condabin:/home/mkandes/.local/bin:/home/mkandes/bin:/cm/shared/apps/sdsc/1.0/bin:/cm/shared/apps/sdsc/1.0/sbin:/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
_LMFILES_=/cm/local/modulefiles/shared:/usr/share/modulefiles/cpu/0.17.3b.lua:/cm/local/modulefiles/slurm/expanse/23.02.7:/cm/shared/modulefiles/sdsc/1.0.lua:/usr/share/modulefiles/DefaultModules.lua
DEBUGINFOD_URLS=https://debuginfod.centos.org/ 
MODULESHOME=/usr/share/lmod/lmod
LMOD_SETTARG_FULL_SUPPORT=no
SDSC_INC=/cm/shared/apps/sdsc/1.0/include
LMOD_PKG=/usr/share/lmod/lmod
LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod
LESSOPEN=||/usr/bin/lesspipe.sh %s
LMOD_FULL_SETTARG_SUPPORT=no
LMOD_DIR=/usr/share/lmod/lmod/libexec
BASH_FUNC_which%%=() {  ( alias;
 eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@
}
BASH_FUNC_module%%=() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)
}
BASH_FUNC_ml%%=() {  eval $($LMOD_DIR/ml_cmd "$@")
}
_=/home/mkandes/software/openmpi/4.1.6/default/gcc/8.5.0/bin/orted
OMPI_MCA_ess_base_jobid=1943601153
OMPI_MCA_ess_base_vpid=0
OMPI_MCA_ess_base_num_procs=2
OMPI_MCA_orte_node_regex=exp-[1:9]-55,exp-[1:9]-56@0(2)
OMPI_MCA_orte_hnp_uri=1943601152.0;tcp://10.21.9.55,10.22.9.55,198.202.101.232:48663
OMPI_MCA_plm=rsh
OMPI_MCA_routed=radix
OMPI_MCA_orte_parent_uri=1943601152.0;tcp://10.21.9.55,10.22.9.55,198.202.101.232:48663
IPATH_NO_BACKTRACE=1
HFI_NO_BACKTRACE=1
OMPI_MCA_orte_local_daemon_uri=1943601152.1;tcp://10.21.9.56,10.22.9.56,198.202.101.231:39241
OMPI_MCA_mpi_oversubscribe=0
OMPI_MCA_orte_app_num=0
OMPI_UNIVERSE_SIZE=129
OMPI_MCA_orte_num_nodes=1
OMPI_MCA_shmem_RUNTIME_QUERY_hint=mmap
OMPI_MCA_orte_bound_at_launch=1
OMPI_MCA_ess=^singleton
OMPI_MCA_orte_ess_num_procs=1
OMPI_COMM_WORLD_SIZE=1
OMPI_COMM_WORLD_LOCAL_SIZE=1
OMPI_MCA_orte_tmpdir_base=/tmp
OMPI_MCA_orte_top_session_dir=/tmp/ompi.exp-9-56.501506
OMPI_MCA_orte_jobfam_session_dir=/tmp/ompi.exp-9-56.501506/jf.29657
OMPI_NUM_APP_CTX=1
OMPI_FIRST_RANKS=0
OMPI_APP_CTX_NUM_PROCS=1
OMPI_MCA_initial_wdir=/home/mkandes
OMPI_MCA_orte_launch=1
PMIX_NAMESPACE=1943601153
PMIX_RANK=0
PMIX_SERVER_URI3=1943601152.1;tcp4://127.0.0.1:38001
PMIX_SERVER_URI2=1943601152.1;tcp4://127.0.0.1:38001
PMIX_SERVER_URI21=1943601152.1;tcp4://127.0.0.1:38001
PMIX_SECURITY_MODE=native
PMIX_PTL_MODULE=tcp,usock
PMIX_BFROP_BUFFER_TYPE=PMIX_BFROP_BUFFER_NON_DESC
PMIX_GDS_MODULE=ds21,ds12,hash
PMIX_SERVER_TMPDIR=/tmp/ompi.exp-9-56.501506/jf.29657
PMIX_SYSTEM_TMPDIR=/tmp
PMIX_DSTORE_21_BASE_PATH=/tmp/ompi.exp-9-56.501506/jf.29657/pmix_dstor_ds21_2398799
PMIX_DSTORE_ESH_BASE_PATH=/tmp/ompi.exp-9-56.501506/jf.29657/pmix_dstor_ds12_2398799
PMIX_HOSTNAME=exp-9-56
PMIX_VERSION=3.2.5a1
OMPI_COMM_WORLD_RANK=0
OMPI_COMM_WORLD_LOCAL_RANK=0
OMPI_COMM_WORLD_NODE_RANK=0
OMPI_MCA_orte_ess_node_rank=0
PMIX_ID=1943601153.0
OMPI_FILE_LOCATION=/tmp/ompi.exp-9-56.501506/jf.29657/0/1
[mkandes@exp-9-55 ~]$
```

### References:
- https://slurm.schedmd.com/salloc.html
- https://docs.open-mpi.org/en/v5.0.x/launching-apps/ssh.html
- https://www.open-mpi.org/faq/?category=running#mpirun-hostfile
- https://www.open-mpi.org/faq/?category=running#mpirun-host
