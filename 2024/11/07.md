```
root@pytorch-240-gaudi-1180-ubuntu-2204-amd64-1h225-h929t:~/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision# python3 train.py --help
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
usage: train.py [-h] [--data-path DATA_PATH] [--dl-time-exclude DL_TIME_EXCLUDE] [--model MODEL] [--device DEVICE] [-b BATCH_SIZE] [--epochs N] [-ebe N] [-eoe N]
                [--dl-worker-type {MP,HABANA}] [-j N] [--process-per-node N] [--hls_type HLS_TYPE] [--lr LR] [--lars_base_learning_rate LARS_BASE_LEARNING_RATE]
                [--lars_end_learning_rate LARS_END_LEARNING_RATE] [--lars_warmup_epochs LARS_WARMUP_EPOCHS] [--lars_decay_epochs LARS_DECAY_EPOCHS] [--momentum M]
                [--wd W] [--lwd W] [--lr-step-size LR_STEP_SIZE] [--custom-lr-values N [N ...]] [--custom-lr-milestones N [N ...]] [--lr-gamma LR_GAMMA]
                [--label-smoothing LABEL_SMOOTHING] [--print-freq PRINT_FREQ] [--output-dir OUTPUT_DIR] [--channels-last CHANNELS_LAST] [--resume RESUME]
                [--start-epoch N] [--seed SEED] [--cache-dataset] [--sync-bn] [--test-only] [--pretrained] [--use_torch_compile] [--no-compiled-autograd]
                [--hpu_graphs HPU_GRAPHS] [--optimizer {lars,sgd}] [--enable-tensorboard-logging] [--force_native_sgd] [--apex] [--apex-opt-level APEX_OPT_LEVEL]
                [--world-size WORLD_SIZE] [--dist-url DIST_URL] [--num-train-steps T] [--num-eval-steps E] [--save-checkpoint] [--save-model]
                [--run-lazy-mode RUN_LAZY_MODE] [--deterministic] [--profile] [--profile_steps PROFILE_STEPS] [--autocast]

PyTorch Classification Training

options:
  -h, --help            show this help message and exit
  --data-path DATA_PATH
                        dataset
  --dl-time-exclude DL_TIME_EXCLUDE
                        Set to False to include data load time
  --model MODEL         select Resnet models from resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x4d, resnext101_32x8d,
                        wide_resnet50_2, wide_resnet101_2
  --device DEVICE       device
  -b BATCH_SIZE, --batch-size BATCH_SIZE
  --epochs N            number of total epochs to run
  -ebe N, --epochs_between_evals N
                        number of epochs to be completed before evaluation (default: 1)
  -eoe N, --eval_offset_epochs N
                        offsets the epoch on which the evaluation starts (default: 0)
  --dl-worker-type {MP,HABANA}
                        select multiprocessing or habana accelerated
  -j N, --workers N     number of data loading workers (default: 10)
  --process-per-node N  Number of process per node
  --hls_type HLS_TYPE   Node type
  --lr LR               initial learning rate
  --lars_base_learning_rate LARS_BASE_LEARNING_RATE
                        base learning rate for lars
  --lars_end_learning_rate LARS_END_LEARNING_RATE
                        end learning rate for lars
  --lars_warmup_epochs LARS_WARMUP_EPOCHS
                        number of warmup epochs for lars
  --lars_decay_epochs LARS_DECAY_EPOCHS
                        number of decay epochs for lars
  --momentum M          momentum
  --wd W, --weight-decay W
                        weight decay (default: 1e-4)
  --lwd W, --lars-weight-decay W
                        lars weight decay (default: 5e-5)
  --lr-step-size LR_STEP_SIZE
                        decrease lr every step-size epochs
  --custom-lr-values N [N ...]
                        custom lr values list
  --custom-lr-milestones N [N ...]
                        custom lr milestones list
  --lr-gamma LR_GAMMA   decrease lr by a factor of lr-gamma
  --label-smoothing LABEL_SMOOTHING
                        Apply label smoothing to the loss. This applies toCrossEntropyLoss, when label_smoothing is greater than 0.
  --print-freq PRINT_FREQ
                        print frequency
  --output-dir OUTPUT_DIR
                        path where to save
  --channels-last CHANNELS_LAST
                        Whether input is in channels last format.Any value other than True(case insensitive) disables channels-last
  --resume RESUME       resume from checkpoint
  --start-epoch N       start epoch
  --seed SEED           random seed
  --cache-dataset       Cache the datasets for quicker initialization. It also serializes the transforms
  --sync-bn             Use sync batch norm
  --test-only           Only test the model
  --pretrained          Use pre-trained models from the modelzoo
  --use_torch_compile   Use torch.compile feature to run the model
  --no-compiled-autograd
                        Disable compiled_autograd for torch.compile
  --hpu_graphs HPU_GRAPHS
                        Use HPU graphs feature to run the model by default
  --optimizer {lars,sgd}
                        Select an optimizer from `lars` or `sgd`
  --enable-tensorboard-logging
                        enable logging using tensorboard things such as accuracy, loss or performance (img/s)
  --force_native_sgd    forces to use native SGD optimizer - to be used only together with --optimizer=sgd
  --apex                Use apex for mixed precision training
  --apex-opt-level APEX_OPT_LEVEL
                        For apex mixed precision trainingO0 for FP32 training, O1 for mixed precision training.For further detail, see
                        https://github.com/NVIDIA/apex/tree/master/examples/imagenet
  --world-size WORLD_SIZE
                        number of distributed processes
  --dist-url DIST_URL   url used to set up distributed training
  --num-train-steps T   number of steps a.k.a iterations to run in training phase
  --num-eval-steps E    number of steps a.k.a iterations to run in evaluation phase
  --save-checkpoint     Whether or not to save checkpoint; True: to save, False to avoid saving. Checkpoint will be stored in {output_dir}/checkpoint.pth, where
                        {output_dir} is the --output_dir argument value.
  --save-model          Whether or not to save model, True: to save, False to avoid saving. Model will be stored in {output_dir}/{model}.model, where {output_dir}
                        and {model} are --output_dir and --model argument values.
  --run-lazy-mode RUN_LAZY_MODE
                        [DEPRECATED] Do not use, it has no effect anymore. Instead, set env variable PT_HPU_LAZY_MODE to 1
  --deterministic       Whether or not to make data loading deterministic;This does not make execution deterministic
  --profile             enable/disable pytorch profiler
  --profile_steps PROFILE_STEPS
                        warmup and active steps when to take profiler. Syntax is x:y where x is warmup steps and y is number of steps for which the profiler will
                        be active
  --autocast            enable autocast mode on Gaudi
root@pytorch-240-gaudi-1180-ubuntu-2204-amd64-1h225-h929t:~/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision#
```
