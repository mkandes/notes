# Voyager - Gaudi2 Testing

At this time, Habana's [torchvision implementation](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/computer_vision/classification/torchvision) of the MLPerf Training benchmark for image classification is failing due to an unknown critical error when run on Gaudi1. It is bleived that this may be a memory-related problem when running multiple instances of the benchmark on a given Voyager node. 

```
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
Namespace(data_path='/scratch/habana-resnet-50-ilsvrc-2012-pt240-g1180-u2204-1h205a-pmmv8', dl_time_exclude=False, model='resnet50', device='hpu', batch_size=256, epochs=90, epochs_between_evals=1, eval_offset_epochs=0, dl_worker_type='HABANA', workers=6, process_per_node=8, hls_type='HLS1', lr=0.1, lars_base_learning_rate=9.0, lars_end_learning_rate=0.0001, lars_warmup_epochs=3, lars_decay_epochs=36, momentum=0.9, weight_decay=0.0001, lars_weight_decay=5e-05, lr_step_size=30, custom_lr_values=[0.1, 0.01, 0.001, 0.0001], custom_lr_milestones=[0, 30, 60, 80], lr_gamma=0.1, label_smoothing=0.0, print_freq=20, output_dir='.', channels_last=False, resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, pretrained=False, use_torch_compile=True, no_compiled_autograd=False, hpu_graphs=True, optimizer='sgd', enable_tensorboard_logging=False, force_native_sgd=False, apex=False, apex_opt_level='O1', world_size=1, dist_url='env://', num_train_steps=9223372036854775807, num_eval_steps=9223372036854775807, save_checkpoint=False, save_model=False, run_lazy_mode=False, deterministic=True, profile=False, profile_steps='0', is_autocast=True, rank=-1, local_rank=-1, distributed=False)
Loading data
Loading training data
Took 2.989082098007202
Loading validation data
Creating samplers
Running with Habana aeon DataLoader
Running with Habana aeon DataLoader
Creating model
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Synapse detected a device critical error that requires a restart. Killing process in 5 seconds (hl: 1) 20:26:34 [please check log files for dfa cause]
/bin/bash: line 44: 54063 Killed                  python3 -u train.py --dl-worker-type 'HABANA' --batch-size 256 --model 'resnet50' --device 'hpu' --workers 6 --print-freq 20 --dl-time-exclude 'False' --deterministic --data-path "${DATASET_DESTINATION_DIR}" --epochs 90 --autocast --lr 0.1 --custom-lr-values 0.1 0.01 0.001 0.0001 --custom-lr-milestones 0 30 60 80 --run-lazy-mode 'False' --use_torch_compile
real 23.68
user 22.26
sys 11.64
```

However, the benchmark run successfully on a single Gaudi2 with different hyperparameter configurations. Only a few runs have failed thus far.

```
Epoch: [0]  [5000/5005]  eta: 0:00:00  lr: 0.1  img/s: 6269.159464984836  loss: 4.0255 (5.2142)  acc1: 19.9219 (9.0234)  acc5: 42.9688 (21.9109)  time: 0.0408  data: 0.0201
Epoch: [0] Total time: 0:03:44
Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic
/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.
  warnings.warn(
/home/mkandes/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision/utils.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  pred_cpu = torch.tensor(pred, device='cpu')
Traceback (most recent call last):
  File "/home/mkandes/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision/train.py", line 701, in <module>
    main(args)
  File "/home/mkandes/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision/train.py", line 514, in main
    evaluate(model_for_eval, criterion, data_loader_test, device=device,
  File "/home/mkandes/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision/train.py", line 145, in evaluate
    acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))
  File "/home/mkandes/software/habana/gaudi/1.18.0/Model-References/PyTorch/computer_vision/classification/torchvision/utils.py", line 184, in accuracy
    pred_cpu = torch.tensor(pred, device='cpu')
RuntimeError: [Rank:0] FATAL ERROR :: MODULE:PT_SYNHELPER workspace Allocation of size ::1645856000 failed!
real 263.28
user 636.33
sys 170.71
```

Unfortuantely, however, the multi-card, single-node configurations is also failing on Gaudi2. 
