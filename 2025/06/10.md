# Setting up sparklyr via jupyter

```
[mkandes@login02 environments]$ galyleo launch --account use300 --partition debug --cpus 8 --memory 16 --conda-yml sparklyr.yaml --mamba --cache
Preparing galyleo for launch into Jupyter orbit ...
Listing all launch parameters ...
  command-line options       : values
    -A | --account           : use300
    -R | --reservation       : 
    -p | --partition         : debug
    -q | --qos               : 
    -N | --nodes             : 1
    -c | --cpus              : 8
    -m | --memory            : 16 GB
    -g | --gpus              : 
       | --gres              : 
    -t | --time-limit        : 00:30:00
    -C | --constraint        : 
    -i | --interface         : lab
    -d | --notebook-dir      : 
       | --scratch-dir       : "/scratch/${USER}/job_${SLURM_JOB_ID}"
    -e | --env-modules       : singularitypro
       | --append-modulepath : 
    -s | --sif               : 
    -B | --bind              : 
       | --nv                : 
       | --conda-init        : 
       | --conda-env         : 
       | --conda-yml         : /home/mkandes/software/miniconda/environments/sparklyr.yaml
       | --conda-version     : latest
       | --mamba             : true
       | --cache             : true
       | --spark-home        : 
       | --disable-checklist : false
       | --checklist-timeout : 10 s
    -Q | --quiet             : 1
Your token is 
elderly-gradually-petal
200
Generating Jupyter launch script ...
Submitted Jupyter launch script to Slurm. Your SLURM_JOB_ID is 40190560.
Success! Token linked to jobid.
Please copy and paste the HTTPS URL provided below into your web browser.
Do not share this URL with others. It is the password to your Jupyter notebook session.
Your Jupyter notebook session will begin once compute resources are allocated to your job by the scheduler.
https://elderly-gradually-petal.expanse-user-content.sdsc.edu/?token=0106124f27b1c87d8843
[mkandes@login02 environments]$ cat sparklyr.yaml 
name: sparklyr
  
channels:
  - conda-forge

dependencies:
  - pyspark
  - r-sparklyr
  - jupyterlab
  - r-irkernel
[mkandes@login02 environments]$
```

```
[mkandes@exp-9-55 ~]$ which spark-submit
/scratch/mkandes/job_40190560/sparklyr/bin/spark-submit
[mkandes@exp-9-55 ~]$ export SPARK_HOME='scratch/mkandes/job_40190560/sparklyr'
[mkandes@exp-9-55 ~]$ R

R version 4.4.3 (2025-02-28) -- "Trophy Case"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-conda-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(sparklyr)

Attaching package: ‘sparklyr’

The following object is masked from ‘package:stats’:

    filter

> sc <- spark_connect(master = "local")
Error in system2(file.path(spark_home, "bin", "spark-submit"), "--version",  : 
  error in running command
> sc <- spark_connect(master = "local")
Error in system2(file.path(spark_home, "bin", "spark-submit"), "--version",  : 
  error in running command
> sc <- spark_connect
spark_connect             spark_connect_method      spark_connection          spark_connection_find     spark_connection_is_open  
> sc <- spark_connect
spark_connect             spark_connect_method      spark_connection          spark_connection_find     spark_connection_is_open  
> sc <- spark_connect(
master=         spark_home=     method=         app_name=       version=        config=         extensions=     packages=       scala_version=  ...
> sc <- spark_connect(master='local')
Error in system2(file.path(spark_home, "bin", "spark-submit"), "--version",  : 
  error in running command
> exit()
Error in exit() : could not find function "exit"
> quit()
Save workspace image? [y/n/c]: n
[mkandes@exp-9-55 ~]$ spark-submit --version
/scratch/mkandes/job_40190560/sparklyr/bin/spark-submit: line 27: /home/mkandes/scratch/mkandes/job_40190560/sparklyr/bin/spark-class: No such file or directory
[mkandes@exp-9-55 ~]$ 
```

```
[mkandes@exp-9-55 ~]$ ls -lahtr /scratch/mkandes/job_40190560/sparklyr/bin/ | grep spark
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 sparkR
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 pyspark.cmd
-rwxr-xr-x  1 mkandes use300 1.5K May 23 00:00 pyspark2.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-submit.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 sparkR.cmd
-rwxr-xr-x  1 mkandes use300 3.4K May 23 00:00 spark-class2.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-shell.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-submit2.cmd
-rwxr-xr-x  1 mkandes use300 2.7K May 23 00:00 load-spark-env.sh
-rwxr-xr-x  1 mkandes use300 3.5K May 23 00:00 spark-class
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 sparkR2.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-class.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-sql.cmd
-rwxr-xr-x  1 mkandes use300 2.6K May 23 00:00 find-spark-home.cmd
-rwxr-xr-x  1 mkandes use300 4.0K May 23 00:00 find_spark_home.py
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-submit
-rwxr-xr-x  1 mkandes use300 4.0K May 23 00:00 pyspark
-rwxr-xr-x  1 mkandes use300 2.4K May 23 00:00 load-spark-env.cmd
-rwxr-xr-x  1 mkandes use300 1.8K May 23 00:00 spark-shell2.cmd
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-sql
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-connect-shell
-rwxr-xr-x  1 mkandes use300 3.3K May 23 00:00 spark-shell
-rwxr-xr-x  1 mkandes use300 1.9K May 23 00:00 find-spark-home
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-sql2.cmd
[mkandes@exp-9-55 ~]$ 
```

```
[mkandes@exp-9-55 ~]$ export SPARK_HOME='/scratch/mkandes/job_40190560/sparklyr'
[mkandes@exp-9-55 ~]$ printenv | grep SPARK_HOME
SPARK_HOME=/scratch/mkandes/job_40190560/sparklyr
[mkandes@exp-9-55 ~]$ R

R version 4.4.3 (2025-02-28) -- "Trophy Case"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-conda-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(sparklyr)

Attaching package: ‘sparklyr’

The following object is masked from ‘package:stats’:

    filter

> sc <- spark_connect(master = "local")
Error in spark_version_from_home(spark_home) : 
  Failed to detect version from SPARK_HOME or SPARK_HOME_VERSION. Try passing the spark version explicitly.
In addition: Warning message:
In system2(file.path(spark_home, "bin", "spark-submit"), "--version",  :
  running command ''/scratch/mkandes/job_40190560/sparklyr/bin/spark-submit' --version 2>&1' had status 1
> 
```

```
[mkandes@exp-9-55 ~]$ spark-submit --version
Failed to find Spark jars directory (/scratch/mkandes/job_40190560/sparklyr/assembly/target/scala-2.13/jars).
You need to build Spark with the target "package" before running this program.
[mkandes@exp-9-55 ~]$ 
```

- https://stackoverflow.com/questions/27618843/why-does-spark-submit-and-spark-shell-fail-with-failed-to-find-spark-assembly-j
- https://stackoverflow.com/questions/66218303/sparklyr-fails-to-download-spark-from-apache-in-dockerfile
- https://github.com/sparklyr/sparklyr/issues/3129

```
[mkandes@exp-9-55 ~]$ R

R version 4.4.3 (2025-02-28) -- "Trophy Case"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-conda-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> install.packages('sparklyr')
--- Please select a CRAN mirror for use in this session ---
Secure CRAN mirrors 

 1: 0-Cloud [https]
 2: Australia (Canberra) [https]
 3: Australia (Melbourne 1) [https]
 4: Australia (Melbourne 2) [https]
 5: Austria (Wien) [https]
 6: Belgium (Brussels) [https]
 7: Brazil (PR) [https]
 8: Brazil (SP 1) [https]
 9: Brazil (SP 2) [https]
10: Bulgaria [https]
11: Canada (MB) [https]
12: Canada (ON 1) [https]
13: Canada (ON 2) [https]
14: Chile (Santiago) [https]
15: China (Beijing 1) [https]
16: China (Beijing 2) [https]
17: China (Beijing 3) [https]
18: China (Hefei) [https]
19: China (Hong Kong) [https]
20: China (Jinan) [https]
21: China (Lanzhou) [https]
22: China (Nanjing) [https]
23: China (Shanghai 2) [https]
24: China (Shenzhen) [https]
25: China (Wuhan) [https]
26: Colombia (Cali) [https]
27: Cyprus [https]
28: Czech Republic [https]
29: Denmark [https]
30: East Asia [https]
31: Ecuador (Cuenca) [https]
32: Finland (Helsinki) [https]
33: France (Lyon 1) [https]
34: France (Lyon 2) [https]
35: France (Paris 1) [https]
36: Germany (Erlangen) [https]
37: Germany (Göttingen) [https]
38: Germany (Leipzig) [https]
39: Germany (Münster) [https]
40: Greece [https]
41: Hungary [https]
42: Iceland [https]
43: India (Bengaluru) [https]
44: India (Bhubaneswar) [https]
45: Indonesia (Banda Aceh) [https]
46: Iran (Mashhad) [https]
47: Italy (Milano) [https]
48: Italy (Padua) [https]
49: Japan (Yonezawa) [https]
50: Korea (Gyeongsan-si) [https]
51: Mexico (Mexico City) [https]
52: Mexico (Texcoco) [https]
53: Morocco [https]
54: Netherlands (Dronten) [https]
55: New Zealand [https]
56: Norway [https]
57: Poland [https]
58: South Africa (Johannesburg) [https]
59: Spain (A Coruña) [https]
60: Spain (Madrid) [https]
61: Sweden (Umeå) [https]
62: Switzerland (Zurich 1) [https]
63: Taiwan (Taipei) [https]
64: Turkey (Denizli) [https]
65: UK (Bristol) [https]
66: UK (London 1) [https]
67: USA (IA) [https]
68: USA (MI) [https]
69: USA (MO) [https]
70: USA (OH) [https]
71: USA (OR) [https]
72: USA (PA 1) [https]
73: USA (TN) [https]
74: USA (UT) [https]
75: United Arab Emirates [https]
76: Uruguay [https]
77: (other mirrors)

Selection: 70
trying URL 'https://cran.case.edu/src/contrib/sparklyr_1.9.0.tar.gz'
Content type 'application/x-gzip' length 3125946 bytes (3.0 MB)
==================================================
downloaded 3.0 MB

* installing *source* package ‘sparklyr’ ...
** package ‘sparklyr’ successfully unpacked and MD5 sums checked
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (sparklyr)

The downloaded source packages are in
        ‘/scratch/mkandes/job_40202928/RtmpXowikY/downloaded_packages’
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
> spark_install()
Error in spark_install() : could not find function "spark_install"
> library(sparklyr)

Attaching package: ‘sparklyr’

The following object is masked from ‘package:stats’:

    filter

> spark_install()
Installing Spark 2.4.3 for Hadoop 2.7 or later.
Downloading from:
- 'https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz'
Installing to:
- '~/spark/spark-2.4.3-bin-hadoop2.7'
trying URL 'https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz'
Content type 'application/x-gzip' length 229988313 bytes (219.3 MB)
==================================================
downloaded 219.3 MB

Installation complete.
> sc <- spark_connect(master='local')
* Using Spark: 2.4.3
Error in spark_connect_gateway(gatewayAddress, gatewayPort, sessionId,  : 
  Gateway in localhost:8880 did not respond.


Try running `options(sparklyr.log.console = TRUE)` followed by `sc <- spark_connect(...)` for more debugging info.
> sc <- spark_connect(master="local")
* Using Spark: 2.4.3
Error in spark_connect_gateway(gatewayAddress, gatewayPort, sessionId,  : 
  Gateway in localhost:8880 did not respond.


Try running `options(sparklyr.log.console = TRUE)` followed by `sc <- spark_connect(...)` for more debugging info.
> options(sparklyr.log.console = TRUE)
> sc <- spark_connect(master="local")
* Using Spark: 2.4.3
log4j:WARN File option not set for appender [localfile].
log4j:WARN Are you using FileAppender instead of ConsoleAppender?
log4j:ERROR Either File or DatePattern options are not set for appender [localfile].
25/06/10 13:03:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
log4j:ERROR No output stream or file set for the appender named [localfile].
Exception in thread "main" java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)[Ljava/lang/Object;
        at sparklyr.Shell$.main(shell.scala:21)
        at sparklyr.Shell$.main(shell.scala:12)
        at sparklyr.Shell.main(shell.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/06/10 13:03:37 INFO ShutdownHookManager: Shutdown hook called
25/06/10 13:03:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a087a31-2153-4b81-8754-379ca29720b9
Error in spark_connect_gateway(gatewayAddress, gatewayPort, sessionId,  : 
  Gateway in localhost:8880 did not respond.


Try running `options(sparklyr.log.console = TRUE)` followed by `sc <- spark_connect(...)` for more debugging info.
> 
```

# Setting up SparkR with jupyter 

```
[mkandes@login02 environments]$ galyleo launch --account use300 --partition debug --cpus 8 --memory 16 --conda-yml sparkr.yaml --mamba --cache
Preparing galyleo for launch into Jupyter orbit ...
Listing all launch parameters ...
  command-line options       : values
    -A | --account           : use300
    -R | --reservation       : 
    -p | --partition         : debug
    -q | --qos               : 
    -N | --nodes             : 1
    -c | --cpus              : 8
    -m | --memory            : 16 GB
    -g | --gpus              : 
       | --gres              : 
    -t | --time-limit        : 00:30:00
    -C | --constraint        : 
    -i | --interface         : lab
    -d | --notebook-dir      : 
       | --scratch-dir       : "/scratch/${USER}/job_${SLURM_JOB_ID}"
    -e | --env-modules       : singularitypro
       | --append-modulepath : 
    -s | --sif               : 
    -B | --bind              : 
       | --nv                : 
       | --conda-init        : 
       | --conda-env         : 
       | --conda-yml         : /home/mkandes/software/miniconda/environments/sparkr.yaml
       | --conda-version     : latest
       | --mamba             : true
       | --cache             : true
       | --spark-home        : 
       | --disable-checklist : false
       | --checklist-timeout : 10 s
    -Q | --quiet             : 1
Your token is 
unify-chevy-untaxed
200
Generating Jupyter launch script ...
md5sum: sparkr.md5: No such file or directory
Submitted Jupyter launch script to Slurm. Your SLURM_JOB_ID is 40190638.
Success! Token linked to jobid.
Please copy and paste the HTTPS URL provided below into your web browser.
Do not share this URL with others. It is the password to your Jupyter notebook session.
Your Jupyter notebook session will begin once compute resources are allocated to your job by the scheduler.
https://unify-chevy-untaxed.expanse-user-content.sdsc.edu/?token=0f3c943da1ad04f2b4b48c5c54db30ec
[mkandes@login02 environments]$ cat sparkr.yaml 
name: sparkr
  
channels:
  - conda-forge

dependencies:
  - r-sparkr
  - jupyterlab
  - r-irkernel
[mkandes@login02 environments]$
```

```
> library(SparkR)
Warning: SparkR is deprecated in Apache Spark 4.0.0 and will be removed in a future release. To continue using Spark in R, we recommend using sparklyr instead: https://spark.posit.co/get-started/

Attaching package: ‘SparkR’

The following objects are masked from ‘package:stats’:

    cov, filter, lag, na.omit, predict, sd, var, window

The following objects are masked from ‘package:base’:

    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

> sparkR.session()
Spark not found in SPARK_HOME: 
Will you download and install (or reuse if it exists) Spark package under the cache [/home/mkandes/.cache/spark]? (y/n): n
Error in sparkCheckInstall(sparkHome, master, deployMode) : 
  Please make sure Spark package is installed in this machine.
  - If there is one, set the path in sparkHome parameter or environment variable SPARK_HOME.
  - If not, you may run install.spark function to do the job.
> 
```

```
[mkandes@login02 environments]$ galyleo launch --account use300 --partition debug --cpus 8 --memory 16 --conda-yml sparkr.yaml --mamba --cache
Preparing galyleo for launch into Jupyter orbit ...
Listing all launch parameters ...
  command-line options       : values
    -A | --account           : use300
    -R | --reservation       : 
    -p | --partition         : debug
    -q | --qos               : 
    -N | --nodes             : 1
    -c | --cpus              : 8
    -m | --memory            : 16 GB
    -g | --gpus              : 
       | --gres              : 
    -t | --time-limit        : 00:30:00
    -C | --constraint        : 
    -i | --interface         : lab
    -d | --notebook-dir      : 
       | --scratch-dir       : "/scratch/${USER}/job_${SLURM_JOB_ID}"
    -e | --env-modules       : singularitypro
       | --append-modulepath : 
    -s | --sif               : 
    -B | --bind              : 
       | --nv                : 
       | --conda-init        : 
       | --conda-env         : 
       | --conda-yml         : /home/mkandes/software/miniconda/environments/sparkr.yaml
       | --conda-version     : latest
       | --mamba             : true
       | --cache             : true
       | --spark-home        : 
       | --disable-checklist : false
       | --checklist-timeout : 10 s
    -Q | --quiet             : 1
Your token is 
drew-thickness-macarena
200
Generating Jupyter launch script ...
md5sum: WARNING: 1 computed checksum did NOT match
Submitted Jupyter launch script to Slurm. Your SLURM_JOB_ID is 40195622.
Success! Token linked to jobid.
Please copy and paste the HTTPS URL provided below into your web browser.
Do not share this URL with others. It is the password to your Jupyter notebook session.
Your Jupyter notebook session will begin once compute resources are allocated to your job by the scheduler.
https://drew-thickness-macarena.expanse-user-content.sdsc.edu/?token=91ff7d89ad2ffa8a165014696a6ee463
[mkandes@login02 environments]$ cat sparkr.yaml 
name: sparkr
  
channels:
  - conda-forge

dependencies:
  - python=3.11
  - pyspark=3.5.5
  - jupyterlab
  - r-sparkr
  - r-irkernel
[mkandes@login02 environments]$
```

