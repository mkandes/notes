# Setting up sparklyr via jupyter

```
[mkandes@login02 environments]$ galyleo launch --account use300 --partition debug --cpus 8 --memory 16 --conda-yml sparklyr.yaml --mamba --cache
Preparing galyleo for launch into Jupyter orbit ...
Listing all launch parameters ...
  command-line options       : values
    -A | --account           : use300
    -R | --reservation       : 
    -p | --partition         : debug
    -q | --qos               : 
    -N | --nodes             : 1
    -c | --cpus              : 8
    -m | --memory            : 16 GB
    -g | --gpus              : 
       | --gres              : 
    -t | --time-limit        : 00:30:00
    -C | --constraint        : 
    -i | --interface         : lab
    -d | --notebook-dir      : 
       | --scratch-dir       : "/scratch/${USER}/job_${SLURM_JOB_ID}"
    -e | --env-modules       : singularitypro
       | --append-modulepath : 
    -s | --sif               : 
    -B | --bind              : 
       | --nv                : 
       | --conda-init        : 
       | --conda-env         : 
       | --conda-yml         : /home/mkandes/software/miniconda/environments/sparklyr.yaml
       | --conda-version     : latest
       | --mamba             : true
       | --cache             : true
       | --spark-home        : 
       | --disable-checklist : false
       | --checklist-timeout : 10 s
    -Q | --quiet             : 1
Your token is 
elderly-gradually-petal
200
Generating Jupyter launch script ...
Submitted Jupyter launch script to Slurm. Your SLURM_JOB_ID is 40190560.
Success! Token linked to jobid.
Please copy and paste the HTTPS URL provided below into your web browser.
Do not share this URL with others. It is the password to your Jupyter notebook session.
Your Jupyter notebook session will begin once compute resources are allocated to your job by the scheduler.
https://elderly-gradually-petal.expanse-user-content.sdsc.edu/?token=0106124f27b1c87d8843
[mkandes@login02 environments]$ cat sparklyr.yaml 
name: sparklyr
  
channels:
  - conda-forge

dependencies:
  - pyspark
  - r-sparklyr
  - jupyterlab
  - r-irkernel
[mkandes@login02 environments]$
```

```
[mkandes@exp-9-55 ~]$ which spark-submit
/scratch/mkandes/job_40190560/sparklyr/bin/spark-submit
[mkandes@exp-9-55 ~]$ export SPARK_HOME='scratch/mkandes/job_40190560/sparklyr'
[mkandes@exp-9-55 ~]$ R

R version 4.4.3 (2025-02-28) -- "Trophy Case"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-conda-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(sparklyr)

Attaching package: ‘sparklyr’

The following object is masked from ‘package:stats’:

    filter

> sc <- spark_connect(master = "local")
Error in system2(file.path(spark_home, "bin", "spark-submit"), "--version",  : 
  error in running command
> sc <- spark_connect(master = "local")
Error in system2(file.path(spark_home, "bin", "spark-submit"), "--version",  : 
  error in running command
> sc <- spark_connect
spark_connect             spark_connect_method      spark_connection          spark_connection_find     spark_connection_is_open  
> sc <- spark_connect
spark_connect             spark_connect_method      spark_connection          spark_connection_find     spark_connection_is_open  
> sc <- spark_connect(
master=         spark_home=     method=         app_name=       version=        config=         extensions=     packages=       scala_version=  ...
> sc <- spark_connect(master='local')
Error in system2(file.path(spark_home, "bin", "spark-submit"), "--version",  : 
  error in running command
> exit()
Error in exit() : could not find function "exit"
> quit()
Save workspace image? [y/n/c]: n
[mkandes@exp-9-55 ~]$ spark-submit --version
/scratch/mkandes/job_40190560/sparklyr/bin/spark-submit: line 27: /home/mkandes/scratch/mkandes/job_40190560/sparklyr/bin/spark-class: No such file or directory
[mkandes@exp-9-55 ~]$ 
```

```
[mkandes@exp-9-55 ~]$ ls -lahtr /scratch/mkandes/job_40190560/sparklyr/bin/ | grep spark
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 sparkR
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 pyspark.cmd
-rwxr-xr-x  1 mkandes use300 1.5K May 23 00:00 pyspark2.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-submit.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 sparkR.cmd
-rwxr-xr-x  1 mkandes use300 3.4K May 23 00:00 spark-class2.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-shell.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-submit2.cmd
-rwxr-xr-x  1 mkandes use300 2.7K May 23 00:00 load-spark-env.sh
-rwxr-xr-x  1 mkandes use300 3.5K May 23 00:00 spark-class
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 sparkR2.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-class.cmd
-rwxr-xr-x  1 mkandes use300 1.2K May 23 00:00 spark-sql.cmd
-rwxr-xr-x  1 mkandes use300 2.6K May 23 00:00 find-spark-home.cmd
-rwxr-xr-x  1 mkandes use300 4.0K May 23 00:00 find_spark_home.py
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-submit
-rwxr-xr-x  1 mkandes use300 4.0K May 23 00:00 pyspark
-rwxr-xr-x  1 mkandes use300 2.4K May 23 00:00 load-spark-env.cmd
-rwxr-xr-x  1 mkandes use300 1.8K May 23 00:00 spark-shell2.cmd
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-sql
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-connect-shell
-rwxr-xr-x  1 mkandes use300 3.3K May 23 00:00 spark-shell
-rwxr-xr-x  1 mkandes use300 1.9K May 23 00:00 find-spark-home
-rwxr-xr-x  1 mkandes use300 1.1K May 23 00:00 spark-sql2.cmd
[mkandes@exp-9-55 ~]$ 
```

```
[mkandes@exp-9-55 ~]$ export SPARK_HOME='/scratch/mkandes/job_40190560/sparklyr'
[mkandes@exp-9-55 ~]$ printenv | grep SPARK_HOME
SPARK_HOME=/scratch/mkandes/job_40190560/sparklyr
[mkandes@exp-9-55 ~]$ R

R version 4.4.3 (2025-02-28) -- "Trophy Case"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-conda-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(sparklyr)

Attaching package: ‘sparklyr’

The following object is masked from ‘package:stats’:

    filter

> sc <- spark_connect(master = "local")
Error in spark_version_from_home(spark_home) : 
  Failed to detect version from SPARK_HOME or SPARK_HOME_VERSION. Try passing the spark version explicitly.
In addition: Warning message:
In system2(file.path(spark_home, "bin", "spark-submit"), "--version",  :
  running command ''/scratch/mkandes/job_40190560/sparklyr/bin/spark-submit' --version 2>&1' had status 1
> 
```

```
[mkandes@exp-9-55 ~]$ spark-submit --version
Failed to find Spark jars directory (/scratch/mkandes/job_40190560/sparklyr/assembly/target/scala-2.13/jars).
You need to build Spark with the target "package" before running this program.
[mkandes@exp-9-55 ~]$ 
```

- https://stackoverflow.com/questions/27618843/why-does-spark-submit-and-spark-shell-fail-with-failed-to-find-spark-assembly-j
- https://stackoverflow.com/questions/66218303/sparklyr-fails-to-download-spark-from-apache-in-dockerfile
- https://github.com/sparklyr/sparklyr/issues/3129

